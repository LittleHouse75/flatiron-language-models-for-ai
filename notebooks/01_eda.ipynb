{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce533175",
   "metadata": {},
   "source": [
    "![Banner](https://github.com/LittleHouse75/flatiron-resources/raw/main/NevitsBanner.png)\n",
    "----\n",
    "# SAMSum Dataset Exploration\n",
    "----\n",
    "\n",
    "This notebook analyzes the SAMSum dataset used throughout the project.\n",
    "\n",
    "It covers:\n",
    "\n",
    "- Structural features (turns, speakers)\n",
    "- Dialogue/summary lengths (chars, words)\n",
    "- Compression ratios\n",
    "- N-gram distributions\n",
    "- Side-by-side comparisons across **train**, **validation**, and **test**\n",
    "\n",
    "All charts and tables can be enabled/disabled via flags.\n",
    "\n",
    "The dataset loading is handled in `src/load_samsum.py` so each notebook stays isolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7fd446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration flags\n",
    "SHOW_CHARTS = True      # Set False when feeding into LLMs\n",
    "SHOW_TABLES = True      # Set False for final commit if you want visuals only\n",
    "NGRAM_SAMPLE_SIZE = 4000\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Allow imports from project root\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Correct refactored location of load_samsum\n",
    "from src.data.load_data import load_samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b24b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = load_samsum()\n",
    "\n",
    "print(len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee683e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dialogue_turns(dialogue: str):\n",
    "    turns = []\n",
    "    for line in dialogue.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if \":\" in line:\n",
    "            speaker, utt = line.split(\":\", 1)\n",
    "            turns.append((speaker.strip(), utt.strip()))\n",
    "        else:\n",
    "            turns.append((\"UNKNOWN\", line))\n",
    "    return turns\n",
    "\n",
    "\n",
    "def add_structure_features(df):\n",
    "    df = df.copy()\n",
    "    n_turns = []\n",
    "    n_speakers = []\n",
    "\n",
    "    for dlg in df[\"dialogue\"]:\n",
    "        turns = parse_dialogue_turns(dlg)\n",
    "        speakers = {spk for spk, _ in turns}\n",
    "        n_turns.append(len(turns))\n",
    "        n_speakers.append(len(speakers))\n",
    "\n",
    "    df[\"n_turns\"] = n_turns\n",
    "    df[\"n_speakers\"] = n_speakers\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_length_features(df, show_empty_examples: bool = True):\n",
    "    \"\"\"\n",
    "    Add length-based features to the dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with 'dialogue' and 'summary' columns\n",
    "    show_empty_examples : bool\n",
    "        If True, display examples of any empty dialogues/summaries found\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Copy of input with additional columns:\n",
    "        - dialogue_char_len, summary_char_len\n",
    "        - dialogue_word_len, summary_word_len\n",
    "        - summary_fraction (NaN for problematic rows)\n",
    "        - has_empty_dialogue, has_empty_summary (boolean flags)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic length features\n",
    "    df[\"dialogue_char_len\"] = df[\"dialogue\"].str.len()\n",
    "    df[\"summary_char_len\"] = df[\"summary\"].str.len()\n",
    "    df[\"dialogue_word_len\"] = df[\"dialogue\"].str.split().str.len()\n",
    "    df[\"summary_word_len\"] = df[\"summary\"].str.split().str.len()\n",
    "    \n",
    "    # Create explicit flags for empty content\n",
    "    df[\"has_empty_dialogue\"] = df[\"dialogue_word_len\"] == 0\n",
    "    df[\"has_empty_summary\"] = df[\"summary_word_len\"] == 0\n",
    "    \n",
    "    # Calculate summary fraction, handling division by zero\n",
    "    # - Empty dialogue (0 words) ‚Üí NaN (can't compute ratio)\n",
    "    # - Empty summary (0 words) ‚Üí NaN (not meaningful)\n",
    "    # - Both empty (0/0) ‚Üí NaN\n",
    "    safe_dialogue_len = df[\"dialogue_word_len\"].replace(0, np.nan)\n",
    "    safe_summary_len = df[\"summary_word_len\"].replace(0, np.nan)\n",
    "    df[\"summary_fraction\"] = safe_summary_len / safe_dialogue_len\n",
    "    \n",
    "    # Detailed reporting\n",
    "    empty_dialogues = df[\"has_empty_dialogue\"].sum()\n",
    "    empty_summaries = df[\"has_empty_summary\"].sum()\n",
    "    both_empty = (df[\"has_empty_dialogue\"] & df[\"has_empty_summary\"]).sum()\n",
    "    \n",
    "    print(f\"  üìä Data Quality Report:\")\n",
    "    print(f\"     Total rows: {len(df)}\")\n",
    "    \n",
    "    if empty_dialogues > 0:\n",
    "        print(f\"     ‚ö†Ô∏è  Empty dialogues (0 words): {empty_dialogues}\")\n",
    "        if show_empty_examples:\n",
    "            empty_dlg_df = df[df[\"has_empty_dialogue\"]][[\"dialogue\", \"summary\"]].head(3)\n",
    "            for idx, row in empty_dlg_df.iterrows():\n",
    "                dlg_preview = repr(row['dialogue'][:50]) if row['dialogue'] else \"''\"\n",
    "                print(f\"        ID {idx}: dialogue={dlg_preview}\")\n",
    "    \n",
    "    if empty_summaries > 0:\n",
    "        print(f\"     ‚ö†Ô∏è  Empty summaries (0 words): {empty_summaries}\")\n",
    "        if show_empty_examples:\n",
    "            empty_sum_df = df[df[\"has_empty_summary\"]][[\"dialogue\", \"summary\"]].head(3)\n",
    "            for idx, row in empty_sum_df.iterrows():\n",
    "                print(f\"        ID {idx}: summary={repr(row['summary'])}\")\n",
    "    \n",
    "    if both_empty > 0:\n",
    "        print(f\"     ‚ö†Ô∏è  Both empty (dialogue AND summary): {both_empty}\")\n",
    "    \n",
    "    total_problematic = (df[\"has_empty_dialogue\"] | df[\"has_empty_summary\"]).sum()\n",
    "    if total_problematic > 0:\n",
    "        pct = total_problematic / len(df) * 100\n",
    "        print(f\"     üìâ Total problematic rows: {total_problematic} ({pct:.2f}%)\")\n",
    "        print(f\"        These rows will have NaN for summary_fraction\")\n",
    "    else:\n",
    "        print(f\"     ‚úì No empty dialogues or summaries found\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516057b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_length_features(add_structure_features(train_df))\n",
    "val   = add_length_features(add_structure_features(val_df))\n",
    "test  = add_length_features(add_structure_features(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723e59ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tables = {\n",
    "    \"train\": train.describe(),\n",
    "    \"validation\": val.describe(),\n",
    "    \"test\": test.describe(),\n",
    "}\n",
    "\n",
    "if SHOW_TABLES:\n",
    "    for name, df in summary_tables.items():\n",
    "        print(f\"\\n=== {name.upper()} ‚Äî DESCRIPTIVE STATS ===\")\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59955c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_CHARTS:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    for ax, df, title in zip(\n",
    "        axes, [train, val, test], [\"Train\", \"Validation\", \"Test\"]\n",
    "    ):\n",
    "        df[\"n_turns\"].plot.hist(ax=ax, bins=40)\n",
    "        ax.set_title(f\"{title}: Turns per dialogue\")\n",
    "    plt.show()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    for ax, df, title in zip(\n",
    "        axes, [train, val, test], [\"Train\", \"Validation\", \"Test\"]\n",
    "    ):\n",
    "        df[\"n_speakers\"].value_counts().sort_index().plot.bar(ax=ax)\n",
    "        ax.set_title(f\"{title}: # Speakers\")\n",
    "    plt.show()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    for ax, df, title in zip(\n",
    "        axes, [train, val, test], [\"Train\", \"Validation\", \"Test\"]\n",
    "    ):\n",
    "        df[\"dialogue_word_len\"].plot.hist(bins=40, ax=ax)\n",
    "        ax.set_title(f\"{title}: Dialogue word length\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d522cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_TABLES:\n",
    "    metrics = [\n",
    "        \"n_turns\",\n",
    "        \"n_speakers\",\n",
    "        \"dialogue_word_len\",\n",
    "        \"summary_word_len\",\n",
    "        \"summary_fraction\",\n",
    "    ]\n",
    "\n",
    "    combined = pd.DataFrame({\n",
    "        \"train_mean\":   train[metrics].mean(),\n",
    "        \"val_mean\":     val[metrics].mean(),\n",
    "        \"test_mean\":    test[metrics].mean(),\n",
    "        \"train_median\": train[metrics].median(),\n",
    "        \"val_median\":   val[metrics].median(),\n",
    "        \"test_median\":  test[metrics].median(),\n",
    "    })\n",
    "\n",
    "    print(\"=== GLOBAL SUMMARY STATS (Train / Val / Test) ===\")\n",
    "    print(\"Rows are metrics; columns are means/medians per split.\\n\")\n",
    "    display(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe087b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_CHARTS:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    datasets = [(\"Train\", train), (\"Validation\", val), (\"Test\", test)]\n",
    "\n",
    "    for ax, (name, df) in zip(axes, datasets):\n",
    "        sample = df.sample(n=min(2000, len(df)), random_state=SEED)\n",
    "        ax.scatter(sample[\"dialogue_word_len\"],\n",
    "                   sample[\"summary_word_len\"],\n",
    "                   alpha=0.3, s=10)\n",
    "        ax.set_title(f\"{name}\")\n",
    "        ax.set_xlabel(\"Dialogue word length\")\n",
    "        ax.set_ylabel(\"Summary word length\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa5004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_TABLES:\n",
    "    print(\"=== DIALOGUE vs SUMMARY WORD LENGTH (SAMPLE POINTS) ===\")\n",
    "    print(\"Each table uses the same random sample as the scatter plots.\\n\")\n",
    "\n",
    "    for name, df in [(\"Train\", train), (\"Validation\", val), (\"Test\", test)]:\n",
    "        sample = df.sample(n=min(2000, len(df)), random_state=SEED)\n",
    "        table = sample[[\"dialogue_word_len\", \"summary_word_len\"]].reset_index(drop=True)\n",
    "\n",
    "        print(f\"\\n--- {name.upper()} ---\")\n",
    "        # limit rows so it‚Äôs not insane to scroll / paste into an LLM\n",
    "        display(table.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c6d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def top_ngrams(\n",
    "    corpus,\n",
    "    ngram_range=(1, 1),\n",
    "    top_k=20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute top-k n-grams from a list of texts.\n",
    "    Returns (ngrams, counts) as two arrays.\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    counts = np.asarray(X.sum(axis=0)).ravel()\n",
    "    \n",
    "    vocab = np.array(vectorizer.get_feature_names_out())\n",
    "    top_idx = counts.argsort()[::-1][:top_k]\n",
    "    \n",
    "    return vocab[top_idx], counts[top_idx]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39bef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_top_ngrams_for_splits(\n",
    "    datasets_dict,        # {\"train\": df, \"val\": df, \"test\": df}\n",
    "    column=\"dialogue\",    # or \"summary\"\n",
    "    ngram_range=(1,1),\n",
    "    top_k=20\n",
    "):\n",
    "    results = {}\n",
    "    for name, df in datasets_dict.items():\n",
    "        corpus = df[column].tolist()\n",
    "        ngrams, counts = top_ngrams(corpus, ngram_range=ngram_range, top_k=top_k)\n",
    "        results[name] = (ngrams, counts)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90af768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ngram_row(ngram_results, title_prefix):\n",
    "    if not SHOW_CHARTS:\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    for ax, (split, (ngrams, counts)) in zip(axes, ngram_results.items()):\n",
    "        y = np.arange(len(ngrams))\n",
    "        ax.barh(y, counts)\n",
    "        ax.set_yticks(y)\n",
    "        ax.set_yticklabels(ngrams)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_title(f\"{title_prefix} ‚Äî {split}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ngram_tables(ngram_results, title_prefix):\n",
    "    if not SHOW_TABLES:\n",
    "        return\n",
    "\n",
    "    for split, (ngrams, counts) in ngram_results.items():\n",
    "        print(f\"\\n=== {title_prefix} ‚Äî {split.upper()} ===\")\n",
    "        display(pd.DataFrame({\"ngram\": ngrams, \"count\": counts}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dict = {\n",
    "    \"train\": train,\n",
    "    \"validation\": val,\n",
    "    \"test\": test\n",
    "}\n",
    "\n",
    "# UNIGRAMS (dialogue)\n",
    "uni_dialogue = compute_top_ngrams_for_splits(datasets_dict, column=\"dialogue\", ngram_range=(1,1))\n",
    "plot_ngram_row(uni_dialogue, \"Unigrams (Dialogue)\")\n",
    "show_ngram_tables(uni_dialogue, \"Unigrams (Dialogue)\")\n",
    "\n",
    "# UNIGRAMS (summary)\n",
    "uni_summary = compute_top_ngrams_for_splits(datasets_dict, column=\"summary\", ngram_range=(1,1))\n",
    "plot_ngram_row(uni_summary, \"Unigrams (Summary)\")\n",
    "show_ngram_tables(uni_summary, \"Unigrams (Summary)\")\n",
    "\n",
    "# BIGRAMS (dialogue)\n",
    "bi_dialogue = compute_top_ngrams_for_splits(datasets_dict, column=\"dialogue\", ngram_range=(2,2))\n",
    "plot_ngram_row(bi_dialogue, \"Bigrams (Dialogue)\")\n",
    "show_ngram_tables(bi_dialogue, \"Bigrams (Dialogue)\")\n",
    "\n",
    "# BIGRAMS (summary)\n",
    "bi_summary = compute_top_ngrams_for_splits(datasets_dict, column=\"summary\", ngram_range=(2,2))\n",
    "plot_ngram_row(bi_summary, \"Bigrams (Summary)\")\n",
    "show_ngram_tables(bi_summary, \"Bigrams (Summary)\")\n",
    "\n",
    "# TRIGRAMS (dialogue)\n",
    "tri_dialogue = compute_top_ngrams_for_splits(datasets_dict, column=\"dialogue\", ngram_range=(3,3))\n",
    "plot_ngram_row(tri_dialogue, \"Trigrams (Dialogue)\")\n",
    "show_ngram_tables(tri_dialogue, \"Trigrams (Dialogue)\")\n",
    "\n",
    "# TRIGRAMS (summary)\n",
    "tri_summary = compute_top_ngrams_for_splits(datasets_dict, column=\"summary\", ngram_range=(3,3))\n",
    "plot_ngram_row(tri_summary, \"Trigrams (Summary)\")\n",
    "show_ngram_tables(tri_summary, \"Trigrams (Summary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60e083d",
   "metadata": {},
   "source": [
    "----\n",
    "# Key Takeaways\n",
    "----\n",
    "\n",
    "### 1. Structure is simple and consistent across the dataset\n",
    "\n",
    "- **Conversations are short-ish:** train split averages ~11 turns per dialogue, with medians around 9‚Äì10.\n",
    "- **Mostly 2‚Äì3 speakers:** almost all dialogues involve two speakers; three is less common.\n",
    "- Validation and test splits show **similar distributions** (checked only for sanity), so no major shift across splits.\n",
    "\n",
    "**Modeling implication (based on train):**  \n",
    "A standard seq2seq model is appropriate. The dataset does not require special handling for long or highly multi-party conversations.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Lengths and compression\n",
    "\n",
    "- **Dialogue length (train):** mean ~90‚Äì95 words, median ~70‚Äì75, with a long tail past 500 words.\n",
    "- **Summary length (train):** mean ~20 words, median ~18 words.\n",
    "- **Summary fraction:** roughly ~0.28‚Äì0.30 on the train set (summaries are about 28-30% as long as dialogues).\n",
    "\n",
    "\n",
    "**Modeling implication (based on train):**\n",
    "- The model must compress chats to roughly **one third or less** of their original size.\n",
    "- A source length around **512 tokens** comfortably covers the long tail in *train*.\n",
    "- A target length around **64‚Äì128 tokens** fits the typical summary length.\n",
    "- For the longest dialogues, consistent truncation matters.\n",
    "\n",
    "Validation/test distributions are shown only to confirm they follow the same shape.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Dialogue vs summary length relationship\n",
    "\n",
    "- In the train split, longer dialogues correlate with longer summaries, but the relationship saturates: summaries rarely exceed 20‚Äì40 words.\n",
    "- Validation/test show the same pattern (again, only checked for similarity).\n",
    "\n",
    "**Modeling implication (based on train):**  \n",
    "It is reasonable to **cap summary length**, since the task does not reward very long outputs even for long inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. N-gram patterns\n",
    "\n",
    "Across splits, patterns align well. In *train*:\n",
    "\n",
    "- **Dialogues:** informal, chatty, full of greetings, questions, and first-person pronouns.\n",
    "- **Summaries:** more abstract, compressed, and action-oriented (‚Äúagrees‚Äù, ‚Äúdecides‚Äù, ‚Äúplans‚Äù), with a shift toward third-person narration.\n",
    "\n",
    "Validation/test n-grams are inspected only to confirm similar distributional behavior.\n",
    "\n",
    "**Modeling implication (based on train):**  \n",
    "The model must learn a **style shift**:\n",
    "- from noisy, multi-speaker, first-person chat  \n",
    "- to clean, concise, third-person summaries that emphasize decisions and events.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Overall\n",
    "\n",
    "- The train set shows stable, well-behaved structure for dialogue summarization.  \n",
    "- Validation/test confirm that the same patterns hold, supporting fair evaluation.\n",
    "- The task is **real compression** with a clear stylistic transformation.\n",
    "- Only the longest dialogues challenge typical max-length settings.\n",
    "\n",
    "Training decisions drawn from this notebook:\n",
    "- `max_source_length` and `max_target_length` derived from **train** length statistics  \n",
    "- beam search kept compact due to short target lengths  \n",
    "- truncation strategy guided by the long-tail examples in **train**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
