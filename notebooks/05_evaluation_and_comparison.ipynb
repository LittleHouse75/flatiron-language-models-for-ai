{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca3f80c",
   "metadata": {},
   "source": [
    "![Banner](https://github.com/LittleHouse75/flatiron-resources/raw/main/NevitsBanner.png)\n",
    "---\n",
    "# Model Evaluation and Comparison\n",
    "### Final Test Set Evaluation Across All Experiments\n",
    "---\n",
    "\n",
    "This notebook provides a fair comparison of all models on the **held-out test set**.\n",
    "\n",
    "We evaluate:\n",
    "- **Experiment 1:** BERT → GPT-2 (custom encoder-decoder)\n",
    "- **Experiment 2:** BART and T5 (pretrained seq2seq)\n",
    "- **Experiment 3:** Frontier LLMs via API\n",
    "\n",
    "All models are evaluated on the exact same test examples using consistent\n",
    "ROUGE computation settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c170ade0",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb87208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d3bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.load_data import load_samsum\n",
    "from src.eval.rouge_eval import compute_rouge_from_lists\n",
    "from src.eval.qualitative import generate_summary\n",
    "from src.utils.logging import heading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b03a74",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Paths to saved models\n",
    "BERT_GPT2_DIR = PROJECT_ROOT / \"models\" / \"bert-gpt2\" / \"best\"\n",
    "BART_DIR = PROJECT_ROOT / \"models\" / \"bart\" / \"best\"\n",
    "T5_DIR = PROJECT_ROOT / \"models\" / \"t5\" / \"best\"\n",
    "\n",
    "# API results from Experiment 3\n",
    "API_RESULTS_DIR = PROJECT_ROOT / \"experiments\" / \"exp3_api_llm_results\"\n",
    "\n",
    "# Output paths\n",
    "RESULTS_DIR = PROJECT_ROOT / \"experiments\" / \"final_comparison\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Evaluation settings\n",
    "MAX_SOURCE_LEN = 512\n",
    "MAX_TARGET_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Which models to evaluate (set False to skip)\n",
    "EVALUATE_MODELS = {\n",
    "    \"bert_gpt2\": True,\n",
    "    \"bart\": True,\n",
    "    \"t5\": True,\n",
    "    \"api_models\": True,  # Will use best API model from Experiment 3\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b117be",
   "metadata": {},
   "source": [
    "## 3. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8141f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"Loading Test Data\")\n",
    "\n",
    "train_df, val_df, test_df = load_samsum()\n",
    "\n",
    "print(f\"Test set size: {len(test_df)} examples\")\n",
    "print(f\"\\nSample dialogue:\")\n",
    "print(test_df.iloc[0][\"dialogue\"][:200] + \"...\")\n",
    "print(f\"\\nSample summary:\")\n",
    "print(test_df.iloc[0][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e73f76",
   "metadata": {},
   "source": [
    "## 4. Build Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bba9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Helper function\n",
    "def evaluate_model_on_test(\n",
    "    model,\n",
    "    encoder_tokenizer,          # Changed: was just \"tokenizer\"\n",
    "    decoder_tokenizer,          # Added: separate decoder tokenizer\n",
    "    test_df,\n",
    "    device,\n",
    "    max_source_len,\n",
    "    max_target_len,\n",
    "    source_prefix=\"\",\n",
    "    model_name=\"model\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate summaries for all test examples and compute ROUGE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : transformers model\n",
    "        The model to evaluate\n",
    "    encoder_tokenizer : tokenizer\n",
    "        Tokenizer for encoding input (dialogue)\n",
    "    decoder_tokenizer : tokenizer\n",
    "        Tokenizer for decoding output (summary)\n",
    "    test_df : pd.DataFrame\n",
    "        Test data with 'dialogue' and 'summary' columns\n",
    "    device : torch.device\n",
    "        CPU or GPU\n",
    "    max_source_len : int\n",
    "        Maximum input length\n",
    "    max_target_len : int\n",
    "        Maximum output length\n",
    "    source_prefix : str\n",
    "        Prefix to add to input (e.g., \"summarize: \" for T5)\n",
    "    model_name : str\n",
    "        Name for progress bar display\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple : (predictions_df, rouge_scores)\n",
    "    \"\"\"\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Validate tokenizer compatibility with model\n",
    "    # =========================================================================\n",
    "    # Check that the decoder tokenizer's vocab size matches the model's output vocab\n",
    "    model_vocab_size = None\n",
    "    \n",
    "    # Different model types store vocab size differently\n",
    "    if hasattr(model.config, 'vocab_size'):\n",
    "        model_vocab_size = model.config.vocab_size\n",
    "    elif hasattr(model.config, 'decoder') and hasattr(model.config.decoder, 'vocab_size'):\n",
    "        model_vocab_size = model.config.decoder.vocab_size\n",
    "    \n",
    "    if model_vocab_size is not None:\n",
    "        tokenizer_vocab_size = len(decoder_tokenizer)\n",
    "        # Allow small differences (some models add special tokens)\n",
    "        if abs(model_vocab_size - tokenizer_vocab_size) > 10:\n",
    "            print(f\"⚠️  WARNING: Vocab size mismatch for {model_name}!\")\n",
    "            print(f\"   Model vocab: {model_vocab_size}, Tokenizer vocab: {tokenizer_vocab_size}\")\n",
    "            print(f\"   This may indicate the wrong tokenizer is being used.\")\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=model_name):\n",
    "        dialogue = row[\"dialogue\"]\n",
    "        reference = row[\"summary\"]\n",
    "        \n",
    "        # Generate summary using the correct tokenizers\n",
    "        pred = generate_summary(\n",
    "            model=model,\n",
    "            encoder_tokenizer=encoder_tokenizer,   # For encoding input\n",
    "            decoder_tokenizer=decoder_tokenizer,   # For decoding output\n",
    "            text=dialogue,\n",
    "            device=device,\n",
    "            max_source_len=max_source_len,\n",
    "            max_target_len=max_target_len,\n",
    "            source_prefix=source_prefix,\n",
    "        )\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        references.append(reference)\n",
    "    \n",
    "    # Compute ROUGE\n",
    "    rouge_scores = compute_rouge_from_lists(predictions, references)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = test_df.copy()\n",
    "    results_df[\"model_prediction\"] = predictions\n",
    "    \n",
    "    return results_df, rouge_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd817ab",
   "metadata": {},
   "source": [
    "## 5. Evaluate BERT-GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b23a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "if EVALUATE_MODELS[\"bert_gpt2\"] and BERT_GPT2_DIR.exists():\n",
    "    heading(\"Evaluating BERT → GPT-2\")\n",
    "    \n",
    "    from transformers import EncoderDecoderModel, BertTokenizer, GPT2Tokenizer\n",
    "    \n",
    "    # Load model\n",
    "    bert_gpt2_model = EncoderDecoderModel.from_pretrained(BERT_GPT2_DIR).to(device)\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(BERT_GPT2_DIR)\n",
    "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained(BERT_GPT2_DIR)\n",
    "    \n",
    "    # Ensure pad token is set for GPT-2\n",
    "    if gpt2_tokenizer.pad_token is None:\n",
    "        gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "    \n",
    "    # NOW we pass BOTH tokenizers correctly\n",
    "    results_df, rouge = evaluate_model_on_test(\n",
    "        model=bert_gpt2_model,\n",
    "        encoder_tokenizer=bert_tokenizer,   # BERT for encoding dialogues\n",
    "        decoder_tokenizer=gpt2_tokenizer,   # GPT-2 for decoding summaries\n",
    "        test_df=test_df,\n",
    "        device=device,\n",
    "        max_source_len=MAX_SOURCE_LEN,\n",
    "        max_target_len=MAX_TARGET_LEN,\n",
    "        source_prefix=\"\",\n",
    "        model_name=\"BERT-GPT2\",\n",
    "    )\n",
    "    \n",
    "    all_results[\"BERT-GPT2\"] = {\n",
    "        \"rouge\": rouge,\n",
    "        \"predictions\": results_df,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nBERT-GPT2 Results:\")\n",
    "    print(f\"  ROUGE-1: {rouge['rouge1']:.4f}\")\n",
    "    print(f\"  ROUGE-2: {rouge['rouge2']:.4f}\")\n",
    "    print(f\"  ROUGE-L: {rouge['rougeL']:.4f}\")\n",
    "    \n",
    "    # Save predictions\n",
    "    results_df.to_csv(RESULTS_DIR / \"bert_gpt2_predictions.csv\", index=False)\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping BERT-GPT2 (not found or disabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241a908",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Evaluate BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50033b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_MODELS[\"bart\"] and BART_DIR.exists():\n",
    "    heading(\"Evaluating BART\")\n",
    "    \n",
    "    from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "    \n",
    "    bart_model = BartForConditionalGeneration.from_pretrained(BART_DIR).to(device)\n",
    "    bart_tokenizer = BartTokenizer.from_pretrained(BART_DIR)\n",
    "    \n",
    "    # BART uses the same tokenizer for both encoder and decoder\n",
    "    results_df, rouge = evaluate_model_on_test(\n",
    "        model=bart_model,\n",
    "        encoder_tokenizer=bart_tokenizer,\n",
    "        decoder_tokenizer=bart_tokenizer,  # Same tokenizer for both\n",
    "        test_df=test_df,\n",
    "        device=device,\n",
    "        max_source_len=MAX_SOURCE_LEN,\n",
    "        max_target_len=MAX_TARGET_LEN,\n",
    "        source_prefix=\"\",\n",
    "        model_name=\"BART\",\n",
    "    )\n",
    "    \n",
    "    all_results[\"BART\"] = {\n",
    "        \"rouge\": rouge,\n",
    "        \"predictions\": results_df,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nBART Results:\")\n",
    "    print(f\"  ROUGE-1: {rouge['rouge1']:.4f}\")\n",
    "    print(f\"  ROUGE-2: {rouge['rouge2']:.4f}\")\n",
    "    print(f\"  ROUGE-L: {rouge['rougeL']:.4f}\")\n",
    "    \n",
    "    results_df.to_csv(RESULTS_DIR / \"bart_predictions.csv\", index=False)\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping BART (not found or disabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dfe17a",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Evaluate T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_MODELS[\"t5\"] and T5_DIR.exists():\n",
    "    heading(\"Evaluating T5\")\n",
    "    \n",
    "    from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "    \n",
    "    t5_model = T5ForConditionalGeneration.from_pretrained(T5_DIR).to(device)\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(T5_DIR)\n",
    "    \n",
    "    # Load the prefix used during training\n",
    "    prefix_path = T5_DIR / \"source_prefix.txt\"\n",
    "    if prefix_path.exists():\n",
    "        t5_prefix = prefix_path.read_text().strip()\n",
    "        print(f\"Using saved prefix: '{t5_prefix}'\")\n",
    "    else:\n",
    "        t5_prefix = \"summarize: \"\n",
    "        print(f\"No prefix file found, using default: '{t5_prefix}'\")\n",
    "    \n",
    "    # T5 uses the same tokenizer for both encoder and decoder\n",
    "    results_df, rouge = evaluate_model_on_test(\n",
    "        model=t5_model,\n",
    "        encoder_tokenizer=t5_tokenizer,\n",
    "        decoder_tokenizer=t5_tokenizer,  # Same tokenizer for both\n",
    "        test_df=test_df,\n",
    "        device=device,\n",
    "        max_source_len=MAX_SOURCE_LEN,\n",
    "        max_target_len=MAX_TARGET_LEN,\n",
    "        source_prefix=t5_prefix,\n",
    "        model_name=\"T5\",\n",
    "    )\n",
    "    \n",
    "    all_results[\"T5\"] = {\n",
    "        \"rouge\": rouge,\n",
    "        \"predictions\": results_df,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nT5 Results:\")\n",
    "    print(f\"  ROUGE-1: {rouge['rouge1']:.4f}\")\n",
    "    print(f\"  ROUGE-2: {rouge['rouge2']:.4f}\")\n",
    "    print(f\"  ROUGE-L: {rouge['rougeL']:.4f}\")\n",
    "    \n",
    "    results_df.to_csv(RESULTS_DIR / \"t5_predictions.csv\", index=False)\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping T5 (not found or disabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4b312",
   "metadata": {},
   "source": [
    "## 8. Evaluate API Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3eedf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_MODELS[\"api_models\"]:\n",
    "    heading(\"Loading API Model Results\")\n",
    "    \n",
    "    # Check what split the API models were evaluated on\n",
    "    api_metadata_path = API_RESULTS_DIR / \"evaluation_metadata.json\"\n",
    "    \n",
    "    if api_metadata_path.exists():\n",
    "        with open(api_metadata_path, 'r') as f:\n",
    "            api_metadata = json.load(f)\n",
    "        api_split = api_metadata.get(\"split_used\", \"unknown\")\n",
    "        api_n_samples = api_metadata.get(\"n_samples\", \"unknown\")\n",
    "        print(f\"API models were evaluated on: {api_split} set ({api_n_samples} samples)\")\n",
    "    else:\n",
    "        api_split = \"unknown\"\n",
    "        print(\"⚠ Could not determine which split API models were evaluated on\")\n",
    "    \n",
    "    # Load the ROUGE summary from Experiment 3\n",
    "    rouge_summary_path = API_RESULTS_DIR / \"rouge_summary.csv\"\n",
    "    \n",
    "    if rouge_summary_path.exists():\n",
    "        api_rouge_df = pd.read_csv(rouge_summary_path, index_col=0)\n",
    "        \n",
    "        # Find the best API model by ROUGE-L\n",
    "        best_api_model = api_rouge_df[\"rougeL\"].idxmax()\n",
    "        best_api_scores = api_rouge_df.loc[best_api_model]\n",
    "        \n",
    "        print(f\"\\nBest API model: {best_api_model}\")\n",
    "        print(f\"  ROUGE-1: {best_api_scores['rouge1']:.4f}\")\n",
    "        print(f\"  ROUGE-2: {best_api_scores['rouge2']:.4f}\")\n",
    "        print(f\"  ROUGE-L: {best_api_scores['rougeL']:.4f}\")\n",
    "        \n",
    "        # CRITICAL: Check if we can fairly compare\n",
    "        if api_split != \"test\":\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"⚠️  WARNING: COMPARISON MAY NOT BE VALID\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"API models were evaluated on '{api_split}' set,\")\n",
    "            print(f\"but local models are evaluated on 'test' set.\")\n",
    "            print(\"\\nTo fix this:\")\n",
    "            print(\"1. Open 04_experiment3_api_models.ipynb\")\n",
    "            print(\"2. Set USE_TEST_SET = True\")\n",
    "            print(\"3. Set RUN_API_CALLS = True\")\n",
    "            print(\"4. Re-run the notebook\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # Mark the comparison as provisional\n",
    "            comparison_note = \"(evaluated on different splits - not directly comparable)\"\n",
    "        else:\n",
    "            comparison_note = \"\"\n",
    "        \n",
    "        all_results[f\"API: {best_api_model}\"] = {\n",
    "            \"rouge\": {\n",
    "                \"rouge1\": best_api_scores[\"rouge1\"],\n",
    "                \"rouge2\": best_api_scores[\"rouge2\"],\n",
    "                \"rougeL\": best_api_scores[\"rougeL\"],\n",
    "                \"rougeLsum\": best_api_scores.get(\"rougeLsum\", np.nan),\n",
    "            },\n",
    "            \"predictions\": None,\n",
    "            \"note\": comparison_note,\n",
    "        }\n",
    "    else:\n",
    "        print(f\"API results not found at {rouge_summary_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c7cf47",
   "metadata": {},
   "source": [
    "## 11. Create Comparision Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b273ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"Final Comparison\")\n",
    "\n",
    "if all_results:\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, result in all_results.items():\n",
    "        rouge = result[\"rouge\"]\n",
    "        \n",
    "        # Count how many examples this model was evaluated on\n",
    "        if result[\"predictions\"] is not None:\n",
    "            n_evaluated = len(result[\"predictions\"])\n",
    "        else:\n",
    "            # For API models loaded from file\n",
    "            n_evaluated = \"see notebook 04\"\n",
    "        \n",
    "        comparison_data.append({\n",
    "            \"Model\": model_name,\n",
    "            \"N_Samples\": n_evaluated,  # NEW: Show sample count\n",
    "            \"ROUGE-1\": rouge[\"rouge1\"],\n",
    "            \"ROUGE-2\": rouge[\"rouge2\"],\n",
    "            \"ROUGE-L\": rouge[\"rougeL\"],\n",
    "            \"Note\": result.get(\"note\", \"\"),  # Any caveats\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values(\"ROUGE-L\", ascending=False)\n",
    "    comparison_df = comparison_df.reset_index(drop=True)\n",
    "    \n",
    "    # Check if all models were evaluated on the same number of examples\n",
    "    sample_counts = comparison_df[\"N_Samples\"].unique()\n",
    "    if len(sample_counts) > 1:\n",
    "        print(\"⚠️  WARNING: Models were evaluated on different numbers of examples!\")\n",
    "        print(f\"   Sample counts: {list(sample_counts)}\")\n",
    "        print(\"   Scores may not be directly comparable.\\n\")\n",
    "    \n",
    "    # Format for display\n",
    "    display_df = comparison_df.copy()\n",
    "    for col in [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]:\n",
    "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINAL MODEL COMPARISON (Sorted by ROUGE-L)\")\n",
    "    print(\"=\" * 70)\n",
    "    display(display_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f085d0",
   "metadata": {},
   "source": [
    "## 12. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36178d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results and len(all_results) > 1:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    heading(\"Visualization\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    models = comparison_df[\"Model\"].tolist()\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    \n",
    "    r1 = comparison_df[\"ROUGE-1\"].tolist()\n",
    "    r2 = comparison_df[\"ROUGE-2\"].tolist()\n",
    "    rL = comparison_df[\"ROUGE-L\"].tolist()\n",
    "    \n",
    "    bars1 = ax.bar(x - width, r1, width, label=\"ROUGE-1\", color=\"#2ecc71\")\n",
    "    bars2 = ax.bar(x, r2, width, label=\"ROUGE-2\", color=\"#3498db\")\n",
    "    bars3 = ax.bar(x + width, rL, width, label=\"ROUGE-L\", color=\"#9b59b6\")\n",
    "    \n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Model Comparison: ROUGE Scores on Test Set\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models, rotation=45, ha=\"right\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2, bars3]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.2f}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3),\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom',\n",
    "                       fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / \"model_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved figure to: {RESULTS_DIR / 'model_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b904072c",
   "metadata": {},
   "source": [
    "## 13. Qualitative Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf94592",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"Qualitative Comparison: Same Examples Across Models\")\n",
    "\n",
    "# Pick 3 random test examples\n",
    "sample_indices = test_df.sample(3, random_state=42).index.tolist()\n",
    "\n",
    "for idx in sample_indices:\n",
    "    row = test_df.loc[idx]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"TEST EXAMPLE (index {idx})\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n[DIALOGUE]\\n{row['dialogue'][:500]}{'...' if len(row['dialogue']) > 500 else ''}\")\n",
    "    print(f\"\\n[HUMAN SUMMARY]\\n{row['summary']}\")\n",
    "    \n",
    "    print(f\"\\n[MODEL PREDICTIONS]\")\n",
    "    for model_name, result in all_results.items():\n",
    "        if result[\"predictions\"] is not None:\n",
    "            pred = result[\"predictions\"].loc[idx, \"model_prediction\"]\n",
    "            print(f\"  {model_name:15s}: {pred}\")\n",
    "        else:\n",
    "            print(f\"  {model_name:15s}: (predictions not available)\")\n",
    "    \n",
    "    print(\"-\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295019be",
   "metadata": {},
   "source": [
    "## 14. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a6870e",
   "metadata": {},
   "source": [
    "TBD\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
