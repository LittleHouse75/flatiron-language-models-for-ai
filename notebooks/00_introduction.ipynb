{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ![Banner](https://github.com/LittleHouse75/flatiron-resources/raw/main/NevitsBanner.png)\n",
    "---\n",
    "# Project Preface & Directory\n",
    "### Dialogue Summarization: From Custom Architectures to Frontier LLMs\n",
    "---\n",
    "\n",
    "## BLUF: The Bottom Line Up Front\n",
    "\n",
    "This project set out to answer a specific engineering question: **What is the most effective way to summarize noisy, informal chat logs in 2025?**\n",
    "\n",
    "The industry trend suggests simply sending everything to a massive frontier model (like GPT-4 or Claude). However, our rigorous benchmarking across three distinct architectural approaches reveals a more nuanced reality.\n",
    "\n",
    "**The Headlines:**\n",
    "\n",
    "1.  **Specialization beats Generalization:** A fine-tuned **BART** model (Experiment 2) significantly outperformed massive frontier LLMs on standard quality metrics (ROUGE). It achieved higher accuracy while being **3x faster** and **infinitely cheaper** at scale than the API comparisons.\n",
    "2.  **Architecture Matters:** Our custom \"Frankenstein\" model (DistilBERT encoder + DistilGPT-2 decoder) proved that simply glueing random components together is inefficient. Without pre-trained cross-attention, the model struggled to learn the task, highlighting exactly *why* models like BART and T5 exist.\n",
    "3.  **The \"API Tax\" is Real:** While zero-shot frontier models (Experiment 3) provided the easiest \"time-to-hello-world,\" they struggled to match the specific terse, outcome-focused style required by the dataset. They are excellent prototypes, but expensive production solutions.\n",
    "\n",
    "Ultimately, this project demonstrates that for high-volume, domain-specific tasks, **a smaller, purpose-built model rooted in your own data is still the engineering gold standard.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro_section",
   "metadata": {},
   "source": [
    "## The Problem: Drowning in Context\n",
    "\n",
    "Modern work effectively happens in chat applications. But chat logs are noisy, informal, and fragmented. Employees spend hours every week scrolling through back-and-forth messages just to answer simple questions like:\n",
    "- *\"What was the final decision?\"*\n",
    "- *\"Who owns the next step?\"*\n",
    "- *\"Did we agree on a time?\"*\n",
    "\n",
    "**The Goal**\n",
    "Build an automated system that ingests raw, messy dialogue and outputs a crisp, third-person summary of the outcome.\n",
    "\n",
    "**The Constraints**\n",
    "- **Inputs:** Short, informal, multi-speaker text (slang, typos, emojis included).\n",
    "- **Outputs:** 1-2 sentence summaries suitable for a notification preview.\n",
    "- **Performance:** Must be fast enough for real-time interaction on standard hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roadmap",
   "metadata": {},
   "source": [
    "## The Project Roadmap\n",
    "\n",
    "We investigated this problem through three escalating levels of complexity. Here is how the notebooks are structured:\n",
    "\n",
    "### **[01_eda.ipynb](./01_eda.ipynb) | Knowing the Data**\n",
    "Before modeling, we must understand the terrain. This notebook explores the **SAMSum dataset**.\n",
    "- **Why it matters:** We discover that this task isn't just extraction; it is **compression** (median 75% reduction) and **style transfer** (from informal first-person to formal third-person). This explains why simple extractive baselines fail.\n",
    "\n",
    "### **[02_experiment1_bert_gpt2.ipynb](./02_experiment1_bert_gpt2.ipynb) | The Frankenstein Architecture**\n",
    "Can we build a summarizer by manually connecting two famous models? We create a custom `EncoderDecoderModel` using **DistilBERT** (to read) and **DistilGPT-2** (to write).\n",
    "- **The Experiment:** We force two models that have never spoken to each other to cooperate via randomly initialized cross-attention layers.\n",
    "- **The Takeaway:** It functions, but demonstrates the high cost of training encoder-decoder alignment from scratch.\n",
    "\n",
    "### **[03_experiment2_bart_t5.ipynb](./03_experiment2_bart_t5.ipynb) | The Specialist Models**\n",
    "We switch to models designed specifically for Sequence-to-Sequence tasks: **BART** (Denoising Autoencoder) and **T5** (Text-to-Text Transfer Transformer).\n",
    "- **The Experiment:** We fine-tune these pretrained powerhouses on the same dataset.\n",
    "- **The Takeaway:** Pre-trained cross-attention provides a dramatic leap in performance, stability, and convergence speed.\n",
    "\n",
    "### **[04_experiment3_api_models.ipynb](./04_experiment3_api_models.ipynb) | The Frontier Giants**\n",
    "We skip training entirely and ask the world's smartest models to do the work via API. We benchmark **GPT-5 Mini, Claude 4.5 Haiku, Gemini 2.5 Flash, Qwen 2.5,** and **Kimi K2**.\n",
    "- **The Experiment:** Can zero-shot intelligence beat a fine-tuned specialist?\n",
    "- **The Takeaway:** These models produce \"vibes-based\" summaries that are fluent but often miss the specific stylistic constraints of the ground truth.\n",
    "\n",
    "### **[05_evaluation_and_conclusions.ipynb](./05_evaluation_and_conclusions.ipynb) | The Verdict**\n",
    "We consolidate all metrics—ROUGE scores, inference latency, API costs, and qualitative error analysis—into a final trade-off analysis.\n",
    "- **The Outcome:** A decision matrix for engineers choosing between accuracy, speed, privacy, and cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "how_to_read",
   "metadata": {},
   "source": [
    "## Guided Reading Strategy\n",
    "\n",
    "*   **For the Data Scientist:** Start with **01_eda** to see the n-gram analysis, then review the architecture code in **02_experiment1**.\n",
    "*   **For the ML Engineer:** Focus on **03_experiment2** vs **04_experiment3** to compare the operational trade-offs of hosting local models vs. hitting external APIs.\n",
    "*   **For the Stakeholder:** Skip straight to **05_evaluation_and_conclusions** for the cost/benefit analysis and final recommendations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
