{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84769fa",
   "metadata": {},
   "source": [
    "![Banner](https://github.com/LittleHouse75/flatiron-resources/raw/main/NevitsBanner.png)\n",
    "---\n",
    "# Experiment 3 — Frontier LLMs via OpenRouter\n",
    "### Zero-Shot Dialogue Summarization Using API Models\n",
    "---\n",
    "\n",
    "This notebook evaluates **frontier large language models** (OpenAI, Anthropic, Google, Mistral, etc.)  \n",
    "via **OpenRouter**, using a *single* API interface.\n",
    "\n",
    "We:\n",
    "- Load the SAMSum validation set  \n",
    "- Sample N examples  \n",
    "- Send them to multiple frontier models  \n",
    "- Score ROUGE  \n",
    "- Save predictions + latencies  \n",
    "- Produce qualitative examples  \n",
    "\n",
    "This establishes the **upper-bound performance baseline** for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef465dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101af05e",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c24c4c",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "**Important:** Set `RUN_API_CALLS` to `False` to load cached results instead of \n",
    "making new API calls. This saves money and time when you just want to analyze\n",
    "previous results.\n",
    "\n",
    "You can also control individual models with the `MODELS_TO_RUN` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c88466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# API CALL FLAGS - Set these to control what runs\n",
    "# =============================================================================\n",
    "\n",
    "# Master switch: Set False to load ALL results from cache (no API calls)\n",
    "RUN_API_CALLS = False\n",
    "\n",
    "# Per-model control: Set individual models to False to skip them\n",
    "# Only matters if RUN_API_CALLS = True\n",
    "MODELS_TO_RUN = {\n",
    "    \"gpt5_nano\":        True,\n",
    "    \"gpt5_mini\":        True,\n",
    "    \"gpt5_full\":        True,\n",
    "    \"gpt_oss_20b\":      True,\n",
    "    \"gpt_oss_120b\":     True,\n",
    "    \"gemini_25_flash\":  True,\n",
    "    \"claude_45_sonnet\": True,\n",
    "    \"qwen25_72b\":       True,\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# EXPERIMENT PARAMETERS\n",
    "# =============================================================================\n",
    "N_SAMPLES = 100       # Number of dialogues to evaluate (cost control)\n",
    "SEED = 42\n",
    "MAX_OUT_TOKENS = 512  # Max tokens for model responses\n",
    "\n",
    "# =============================================================================\n",
    "# PATHS\n",
    "# =============================================================================\n",
    "RESULTS_DIR = PROJECT_ROOT / \"experiments\" / \"exp3_api_llm_results\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ROUGE_SUMMARY_PATH = RESULTS_DIR / \"rouge_summary.csv\"\n",
    "LATENCY_SUMMARY_PATH = RESULTS_DIR / \"latency_summary.csv\"\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL DEFINITIONS\n",
    "# =============================================================================\n",
    "OPENROUTER_MODELS = {\n",
    "    # OpenAI family – small → big\n",
    "    \"gpt5_nano\":       \"openai/gpt-5-nano\",\n",
    "    \"gpt5_mini\":       \"openai/gpt-5-mini\",\n",
    "    \"gpt5_full\":       \"openai/gpt-5\",          # flagship upper bound\n",
    "\n",
    "    # OpenAI open-weight models\n",
    "    \"gpt_oss_20b\":     \"openai/gpt-oss-20b\",\n",
    "    \"gpt_oss_120b\":    \"openai/gpt-oss-120b\",\n",
    "\n",
    "    # Google Gemini – fast, very strong general model\n",
    "    \"gemini_25_flash\": \"google/gemini-2.5-flash\",\n",
    "\n",
    "    # Anthropic Claude – strong competitor\n",
    "    \"claude_45_sonnet\": \"anthropic/claude-4.5-sonnet-20250929\",\n",
    "\n",
    "    # Qwen – top-tier open(-ish) model\n",
    "    \"qwen25_72b\":      \"qwen/qwen-2.5-72b-instruct\",\n",
    "}\n",
    "\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"RUN_API_CALLS: {RUN_API_CALLS}\")\n",
    "print(f\"N_SAMPLES: {N_SAMPLES}\")\n",
    "print(f\"Models configured: {len(OPENROUTER_MODELS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2927c5e",
   "metadata": {},
   "source": [
    "## 3. Load SAMSum Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.load_data import load_samsum\n",
    "\n",
    "train_df, val_df, test_df = load_samsum()\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7129afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the evaluation set\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "if N_SAMPLES >= len(val_df):\n",
    "    eval_df = val_df.copy().reset_index(drop=True)\n",
    "    print(f\"Using full validation set: {len(eval_df)} examples\")\n",
    "else:\n",
    "    eval_df = val_df.sample(n=N_SAMPLES, random_state=SEED).reset_index(drop=True)\n",
    "    print(f\"Sampled {len(eval_df)} examples from validation set\")\n",
    "\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3585c52",
   "metadata": {},
   "source": [
    "## 4. Prompt Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15285819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summarization_prompt(dialogue: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a zero-shot summarization prompt for frontier LLMs.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"Summarize the following conversation in 1–2 sentences. \"\n",
    "        \"Keep it brief—aim for 15–30 words. \"\n",
    "        \"Focus on the main point, decisions, requests, or outcomes. \"\n",
    "        \"Ignore small talk and do not add details that aren't supported by the text.\\n\\n\"\n",
    "        \"DIALOGUE:\\n\"\n",
    "        \"-----\\n\"\n",
    "        f\"{dialogue}\\n\"\n",
    "        \"-----\\n\\n\"\n",
    "        \"SUMMARY:\"\n",
    "    )\n",
    "\n",
    "# Test the prompt\n",
    "print(\"Example prompt (truncated):\")\n",
    "print(build_summarization_prompt(eval_df['dialogue'].iloc[0])[:400] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2a04b",
   "metadata": {},
   "source": [
    "## 5. Run API Calls or Load Cached Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.logging import heading\n",
    "\n",
    "def get_cached_result_path(model_label: str) -> Path:\n",
    "    \"\"\"Get the path where results for a model would be cached.\"\"\"\n",
    "    return RESULTS_DIR / f\"{model_label}.csv\"\n",
    "\n",
    "\n",
    "def load_cached_results(model_label: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Load cached results for a model if they exist.\"\"\"\n",
    "    path = get_cached_result_path(model_label)\n",
    "    if path.exists():\n",
    "        return pd.read_csv(path)\n",
    "    return None\n",
    "\n",
    "\n",
    "def run_model_evaluation(model_label: str, model_id: str, eval_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Run API calls for a single model and return results DataFrame.\"\"\"\n",
    "    from tqdm.auto import tqdm\n",
    "    import src.utils.openrouter_client as openrouter_client\n",
    "    \n",
    "    rows = []\n",
    "    for i, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc=model_label):\n",
    "        dialogue = row[\"dialogue\"]\n",
    "        reference = row[\"summary\"]\n",
    "        prompt = build_summarization_prompt(dialogue)\n",
    "        \n",
    "        try:\n",
    "            pred, latency = openrouter_client.call_openrouter_llm(\n",
    "                model=model_id,\n",
    "                prompt=prompt,\n",
    "                max_tokens=MAX_OUT_TOKENS,\n",
    "                temperature=0.2,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            pred = f\"[ERROR: {e}]\"\n",
    "            latency = np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"dialogue\": dialogue,\n",
    "            \"reference_summary\": reference,\n",
    "            \"model_summary\": pred,\n",
    "            \"latency_seconds\": latency,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd6868",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"Loading/Running Model Evaluations\")\n",
    "\n",
    "results_by_model = {}\n",
    "models_run = []\n",
    "models_loaded = []\n",
    "models_skipped = []\n",
    "\n",
    "for label, model_id in OPENROUTER_MODELS.items():\n",
    "    cache_path = get_cached_result_path(label)\n",
    "    \n",
    "    if RUN_API_CALLS and MODELS_TO_RUN.get(label, True):\n",
    "        # Check if we should skip because results already exist\n",
    "        if cache_path.exists():\n",
    "            print(f\"\\n{label}: Cache exists. Re-running anyway (RUN_API_CALLS=True)\")\n",
    "        else:\n",
    "            print(f\"\\n{label}: Running API calls...\")\n",
    "        \n",
    "        # Run the evaluation\n",
    "        df_out = run_model_evaluation(label, model_id, eval_df)\n",
    "        \n",
    "        # Save to cache\n",
    "        df_out.to_csv(cache_path, index=False)\n",
    "        print(f\"  ✓ Saved {len(df_out)} results to {cache_path.name}\")\n",
    "        \n",
    "        results_by_model[label] = df_out\n",
    "        models_run.append(label)\n",
    "        \n",
    "    else:\n",
    "        # Try to load from cache\n",
    "        cached = load_cached_results(label)\n",
    "        \n",
    "        if cached is not None:\n",
    "            results_by_model[label] = cached\n",
    "            models_loaded.append(label)\n",
    "            print(f\"{label}: Loaded {len(cached)} cached results\")\n",
    "        else:\n",
    "            models_skipped.append(label)\n",
    "            print(f\"{label}: No cached results found (skipping)\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY:\")\n",
    "print(f\"  Models run (API calls):  {len(models_run)} - {models_run}\")\n",
    "print(f\"  Models loaded (cache):   {len(models_loaded)} - {models_loaded}\")\n",
    "print(f\"  Models skipped:          {len(models_skipped)} - {models_skipped}\")\n",
    "print(f\"  Total models available:  {len(results_by_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9144c60",
   "metadata": {},
   "source": [
    "## 6. Compute ROUGE Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab5e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"ROUGE Evaluation\")\n",
    "\n",
    "from src.eval.rouge_eval import compute_rouge_from_df\n",
    "\n",
    "rouge_scores = {}\n",
    "\n",
    "for label, df in results_by_model.items():\n",
    "    # Skip if there were errors in all responses\n",
    "    valid_responses = df[~df[\"model_summary\"].str.startswith(\"[ERROR\")]\n",
    "    \n",
    "    if len(valid_responses) == 0:\n",
    "        print(f\"{label}: No valid responses to evaluate\")\n",
    "        continue\n",
    "    \n",
    "    if len(valid_responses) < len(df):\n",
    "        print(f\"{label}: {len(df) - len(valid_responses)} errors, evaluating {len(valid_responses)} valid responses\")\n",
    "    \n",
    "    scores = compute_rouge_from_df(valid_responses)\n",
    "    rouge_scores[label] = scores\n",
    "\n",
    "# Create summary DataFrame\n",
    "if rouge_scores:\n",
    "    rouge_df = pd.DataFrame.from_dict(rouge_scores, orient=\"index\")\n",
    "    rouge_df.index.name = \"model\"\n",
    "    rouge_df = rouge_df.sort_values(by=\"rougeL\", ascending=False)\n",
    "    \n",
    "    # Save summary\n",
    "    rouge_df.to_csv(ROUGE_SUMMARY_PATH)\n",
    "    print(f\"\\nSaved ROUGE summary to: {ROUGE_SUMMARY_PATH}\")\n",
    "    \n",
    "    display(rouge_df)\n",
    "else:\n",
    "    rouge_df = None\n",
    "    print(\"No ROUGE scores to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5389090",
   "metadata": {},
   "source": [
    "## 7. Latency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4723cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"Latency Analysis\")\n",
    "\n",
    "latency_stats = {}\n",
    "\n",
    "for label, df in results_by_model.items():\n",
    "    vals = df[\"latency_seconds\"].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    if len(vals) == 0:\n",
    "        continue\n",
    "        \n",
    "    desc = vals.describe(percentiles=[0.5, 0.9, 0.95])\n",
    "    latency_stats[label] = desc\n",
    "\n",
    "if latency_stats:\n",
    "    latency_df = pd.DataFrame(latency_stats).T\n",
    "    latency_df.index.name = \"model\"\n",
    "    \n",
    "    # Prettier column names\n",
    "    latency_df = latency_df.rename(\n",
    "        columns={\n",
    "            \"count\": \"count\",\n",
    "            \"mean\": \"mean\",\n",
    "            \"std\": \"std\",\n",
    "            \"min\": \"min\",\n",
    "            \"50%\": \"p50\",\n",
    "            \"90%\": \"p90\",\n",
    "            \"95%\": \"p95\",\n",
    "            \"max\": \"max\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Sort by mean latency (fastest first)\n",
    "    latency_df = latency_df.sort_values(by=\"mean\", ascending=True)\n",
    "    \n",
    "    # Save summary\n",
    "    latency_df.to_csv(LATENCY_SUMMARY_PATH)\n",
    "    print(f\"Saved latency summary to: {LATENCY_SUMMARY_PATH}\")\n",
    "    \n",
    "    display(latency_df)\n",
    "else:\n",
    "    latency_df = None\n",
    "    print(\"No latency data to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a81b0",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rouge_df is not None and len(rouge_df) > 0:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # ROUGE scores bar chart\n",
    "    ax = axes[0]\n",
    "    rouge_plot_df = rouge_df[[\"rouge1\", \"rouge2\", \"rougeL\"]].sort_values(\"rougeL\", ascending=True)\n",
    "    rouge_plot_df.plot(kind=\"barh\", ax=ax)\n",
    "    ax.set_xlabel(\"Score\")\n",
    "    ax.set_title(\"ROUGE Scores by Model\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "    \n",
    "    # Latency bar chart\n",
    "    ax = axes[1]\n",
    "    if latency_df is not None and len(latency_df) > 0:\n",
    "        latency_plot = latency_df[\"mean\"].sort_values(ascending=True)\n",
    "        latency_plot.plot(kind=\"barh\", ax=ax, color=\"coral\")\n",
    "        ax.set_xlabel(\"Mean Latency (seconds)\")\n",
    "        ax.set_title(\"API Latency by Model\")\n",
    "        ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"No latency data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "        ax.set_title(\"API Latency by Model\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / \"rouge_latency_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved figure to: {RESULTS_DIR / 'rouge_latency_comparison.png'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c324066f",
   "metadata": {},
   "source": [
    "## 9. Qualitative Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0631b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"Qualitative Examples\")\n",
    "\n",
    "def show_examples(df, model_label, n=5, max_chars=600):\n",
    "    \"\"\"Show n random examples from a model's results.\"\"\"\n",
    "    # Filter out errors\n",
    "    valid_df = df[~df[\"model_summary\"].str.startswith(\"[ERROR\")]\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        print(f\"No valid examples for {model_label}\")\n",
    "        return\n",
    "    \n",
    "    sample = valid_df.sample(n=min(n, len(valid_df)), random_state=SEED)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Examples for: {model_label}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for _, row in sample.iterrows():\n",
    "        d = row[\"dialogue\"]\n",
    "        d = d[:max_chars] + \" ... [truncated]\" if len(d) > max_chars else d\n",
    "        \n",
    "        print(\"\\n--- Example ---\")\n",
    "        print(f\"[DIALOGUE]\\n{d}\")\n",
    "        print(f\"\\n[HUMAN SUMMARY]\\n{row['reference_summary']}\")\n",
    "        print(f\"\\n[MODEL SUMMARY]\\n{row['model_summary']}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# Show examples for each model\n",
    "for label, df in results_by_model.items():\n",
    "    show_examples(df, label, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903bb9b1",
   "metadata": {},
   "source": [
    "## 10. Side-by-Side Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"Side-by-Side Comparison (Same Examples)\")\n",
    "\n",
    "# Pick a few examples to compare across all models\n",
    "if len(results_by_model) > 1:\n",
    "    # Get the first model's dataframe to sample indices\n",
    "    first_model = list(results_by_model.keys())[0]\n",
    "    first_df = results_by_model[first_model]\n",
    "    \n",
    "    # Sample 3 indices\n",
    "    sample_indices = first_df.sample(n=min(3, len(first_df)), random_state=SEED).index.tolist()\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EXAMPLE (index {idx})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Get dialogue and reference from first model (they're the same across all)\n",
    "        row = first_df.loc[idx]\n",
    "        dialogue = row[\"dialogue\"]\n",
    "        reference = row[\"reference_summary\"]\n",
    "        \n",
    "        print(f\"\\n[DIALOGUE]\\n{dialogue[:500]}{'...' if len(dialogue) > 500 else ''}\")\n",
    "        print(f\"\\n[HUMAN SUMMARY]\\n{reference}\")\n",
    "        print(f\"\\n[MODEL SUMMARIES]\")\n",
    "        \n",
    "        for label, df in results_by_model.items():\n",
    "            if idx in df.index:\n",
    "                model_summary = df.loc[idx, \"model_summary\"]\n",
    "                # Truncate long summaries\n",
    "                if len(model_summary) > 200:\n",
    "                    model_summary = model_summary[:200] + \"...\"\n",
    "                print(f\"  {label:20s}: {model_summary}\")\n",
    "        \n",
    "        print(\"-\" * 70)\n",
    "else:\n",
    "    print(\"Need at least 2 models for side-by-side comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a13f8de",
   "metadata": {},
   "source": [
    "## 11. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e2140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"Error Analysis\")\n",
    "\n",
    "error_summary = []\n",
    "\n",
    "for label, df in results_by_model.items():\n",
    "    total = len(df)\n",
    "    errors = df[\"model_summary\"].str.startswith(\"[ERROR\").sum()\n",
    "    error_rate = errors / total * 100 if total > 0 else 0\n",
    "    \n",
    "    error_summary.append({\n",
    "        \"model\": label,\n",
    "        \"total_requests\": total,\n",
    "        \"errors\": errors,\n",
    "        \"error_rate_pct\": f\"{error_rate:.1f}%\",\n",
    "    })\n",
    "    \n",
    "    # Show sample errors if any\n",
    "    if errors > 0:\n",
    "        error_samples = df[df[\"model_summary\"].str.startswith(\"[ERROR\")][\"model_summary\"].head(2).tolist()\n",
    "        print(f\"{label}: {errors} errors ({error_rate:.1f}%)\")\n",
    "        for err in error_samples:\n",
    "            print(f\"  - {err[:100]}...\")\n",
    "\n",
    "error_df = pd.DataFrame(error_summary)\n",
    "display(error_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f11749",
   "metadata": {},
   "source": [
    "## 12. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edce606",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"Final Summary\")\n",
    "\n",
    "if rouge_df is not None and latency_df is not None:\n",
    "    # Combine ROUGE and latency into one summary\n",
    "    summary_data = []\n",
    "    \n",
    "    for model in rouge_df.index:\n",
    "        row = {\n",
    "            \"Model\": model,\n",
    "            \"ROUGE-1\": f\"{rouge_df.loc[model, 'rouge1']:.3f}\",\n",
    "            \"ROUGE-2\": f\"{rouge_df.loc[model, 'rouge2']:.3f}\",\n",
    "            \"ROUGE-L\": f\"{rouge_df.loc[model, 'rougeL']:.3f}\",\n",
    "        }\n",
    "        \n",
    "        if model in latency_df.index:\n",
    "            row[\"Mean Latency (s)\"] = f\"{latency_df.loc[model, 'mean']:.2f}\"\n",
    "            row[\"P95 Latency (s)\"] = f\"{latency_df.loc[model, 'p95']:.2f}\"\n",
    "        else:\n",
    "            row[\"Mean Latency (s)\"] = \"N/A\"\n",
    "            row[\"P95 Latency (s)\"] = \"N/A\"\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    final_summary_df = pd.DataFrame(summary_data)\n",
    "    final_summary_df = final_summary_df.sort_values(\"ROUGE-L\", ascending=False)\n",
    "    \n",
    "    # Save final summary\n",
    "    final_summary_path = RESULTS_DIR / \"final_summary.csv\"\n",
    "    final_summary_df.to_csv(final_summary_path, index=False)\n",
    "    print(f\"Saved final summary to: {final_summary_path}\")\n",
    "    \n",
    "    display(final_summary_df)\n",
    "else:\n",
    "    print(\"Insufficient data for final summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc290f3",
   "metadata": {},
   "source": [
    "---\n",
    "# 13. Key Takeaways — Experiment 3 (Frontier LLMs via OpenRouter)\n",
    "---\n",
    "\n",
    "*Fill this in after running:*\n",
    "\n",
    "**ROUGE Performance:**\n",
    "- Best performing model: [TBD]\n",
    "- Worst performing model: [TBD]\n",
    "- Score range: [TBD]\n",
    "\n",
    "**Latency:**\n",
    "- Fastest model: [TBD]\n",
    "- Slowest model: [TBD]\n",
    "- Latency range: [TBD]\n",
    "\n",
    "**Quality Observations:**\n",
    "- Style differences (concise vs narrative): [TBD]\n",
    "- Error cases or hallucinations: [TBD]\n",
    "- Which model follows instructions best: [TBD]\n",
    "\n",
    "**Cost/Latency Trade-offs:**\n",
    "- Best value (quality/latency ratio): [TBD]\n",
    "- Comparison to local fine-tuned models: [TBD]\n",
    "\n",
    "**Recommendation for Final Comparison:**\n",
    "- Model to use as \"frontier baseline\": [TBD]\n",
    "- Reasoning: [TBD]\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
