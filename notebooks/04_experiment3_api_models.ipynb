{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84769fa",
   "metadata": {},
   "source": [
    "![Banner](https://github.com/LittleHouse75/flatiron-resources/raw/main/NevitsBanner.png)\n",
    "---\n",
    "# Experiment 3 — Frontier LLMs via OpenRouter\n",
    "### Zero-Shot Dialogue Summarization Using API Models\n",
    "---\n",
    "\n",
    "This notebook evaluates **frontier large language models** (OpenAI, Anthropic, Google, Mistral, etc.)  \n",
    "via **OpenRouter**, using a *single* API interface.\n",
    "\n",
    "We:\n",
    "- Load the SAMSum validation set  \n",
    "- Sample N examples  \n",
    "- Send them to multiple frontier models  \n",
    "- Score ROUGE  \n",
    "- Save predictions + latencies  \n",
    "- Produce qualitative examples  \n",
    "\n",
    "This establishes the **upper-bound performance baseline** for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef465dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "RESULTS_DIR = PROJECT_ROOT / \"experiments\" / \"exp3_api_llm_results\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b91011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "N_SAMPLES = 1       # cost control\n",
    "SEED = 42\n",
    "\n",
    "# Summary length for the frontier models\n",
    "MAX_OUT_TOKENS = 128\n",
    "\n",
    "# Which OpenRouter models to evaluate\n",
    "OPENROUTER_MODELS = {\n",
    "    \"gpt5_nano\": \"openai/gpt-5-nano\",\n",
    "    \"gpt5_mini\": \"openai/gpt-5-mini\",\n",
    "    \"gpt_oss_120b\": \"openai/gpt-oss-120b\",\n",
    "    \"gpt_oss_20b\": \"openai/gpt-oss-20b\",\n",
    "}\n",
    "\n",
    "OPENROUTER_MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.load_data import load_samsum\n",
    "\n",
    "train_df, val_df, test_df = load_samsum()\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15285819",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "if N_SAMPLES >= len(val_df):\n",
    "    eval_df = val_df.copy().reset_index(drop=True)\n",
    "else:\n",
    "    eval_df = val_df.sample(n=N_SAMPLES, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b826223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summarization_prompt(dialogue: str) -> str:\n",
    "    return (\n",
    "        \"Summarize the following chat conversation in 1–3 sentences. \"\n",
    "        \"Focus on actions, decisions, and plans. \"\n",
    "        \"Do not add information not supported by the text.\\n\\n\"\n",
    "        \"DIALOGUE:\\n\"\n",
    "        \"-----\\n\"\n",
    "        f\"{dialogue}\\n\"\n",
    "        \"-----\\n\\n\"\n",
    "        \"SUMMARY:\"\n",
    "    )\n",
    "\n",
    "build_summarization_prompt(eval_df['dialogue'].iloc[0])[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a373b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.utils.openrouter_client as openrouter_client\n",
    "importlib.reload(openrouter_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5b037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "results_by_model = {}\n",
    "\n",
    "for label, model_id in OPENROUTER_MODELS.items():\n",
    "    rows = []\n",
    "    for i, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc=label):\n",
    "        dialogue = row[\"dialogue\"]\n",
    "        reference = row[\"summary\"]\n",
    "\n",
    "        prompt = build_summarization_prompt(dialogue)\n",
    "        \n",
    "        try:\n",
    "            pred, latency = openrouter_client.call_openrouter_llm(\n",
    "                model=model_id,\n",
    "                prompt=prompt,\n",
    "                max_tokens=MAX_OUT_TOKENS,\n",
    "                temperature=0.2,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            pred = f\"[ERROR: {e}]\"\n",
    "            latency = np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"dialogue\": dialogue,\n",
    "            \"reference_summary\": reference,\n",
    "            \"model_summary\": pred,\n",
    "            \"latency_seconds\": latency,\n",
    "        })\n",
    "    \n",
    "    df_out = pd.DataFrame(rows)\n",
    "    results_by_model[label] = df_out\n",
    "    df_out.to_csv(RESULTS_DIR / f\"{label}.csv\", index=False)\n",
    "\n",
    "results_by_model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab5e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.rouge_eval import compute_rouge_from_df\n",
    "\n",
    "rouge_scores = {}\n",
    "\n",
    "for label, df in results_by_model.items():\n",
    "    print(f\"\\n=== ROUGE for {label} ===\")\n",
    "    scores = compute_rouge_from_df(df)\n",
    "    rouge_scores[label] = scores\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4723cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latency_summary(df, label):\n",
    "    print(f\"\\n=== Latency stats: {label} ===\")\n",
    "    vals = df[\"latency_seconds\"].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    print(vals.describe(percentiles=[0.5, 0.9, 0.95]))\n",
    "\n",
    "for label, df in results_by_model.items():\n",
    "    latency_summary(df, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed633773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(df, n=5, max_chars=600):\n",
    "    sample = df.sample(n=min(n, len(df)), random_state=SEED)\n",
    "    for _, row in sample.iterrows():\n",
    "        d = row[\"dialogue\"]\n",
    "        d = d[:max_chars] + \" ... [truncated]\" if len(d) > max_chars else d\n",
    "        \n",
    "        print(\"\\n=== Example ===\")\n",
    "        print(\"[DIALOGUE]\")\n",
    "        print(d)\n",
    "        print(\"\\n[HUMAN SUMMARY]\")\n",
    "        print(row[\"reference_summary\"])\n",
    "        print(\"\\n[MODEL SUMMARY]\")\n",
    "        print(row[\"model_summary\"])\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "for label, df in results_by_model.items():\n",
    "    print(f\"\\n##### Examples for {label} #####\")\n",
    "    show_examples(df, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe11ff",
   "metadata": {},
   "source": [
    "---\n",
    "# Key Takeaways — Experiment 3 (Frontier LLMs via OpenRouter)\n",
    "---\n",
    "\n",
    "Fill this in after running:\n",
    "\n",
    "- ROUGE performance of each frontier LLM  \n",
    "- Latency comparisons  \n",
    "- Style differences (concise vs narrative)  \n",
    "- Error cases or hallucinations  \n",
    "- Cost/latency trade-offs compared to local models  \n",
    "- Which model will be used in the final comparison notebook  \n",
    "\n",
    "This experiment establishes the upper-bound performance for the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rocm312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
