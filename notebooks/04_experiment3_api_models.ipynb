{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84769fa",
   "metadata": {},
   "source": [
    "![Banner](https://github.com/LittleHouse75/flatiron-resources/raw/main/NevitsBanner.png)\n",
    "---\n",
    "# Experiment 3 ‚Äî Frontier LLMs via OpenRouter\n",
    "### Zero-Shot Dialogue Summarization Using API Models\n",
    "---\n",
    "\n",
    "This notebook evaluates **frontier large language models** (OpenAI, Anthropic, Google, Mistral, etc.)  \n",
    "via **OpenRouter**, using a *single* API interface.\n",
    "\n",
    "We:\n",
    "- Load the SAMSum dataset (test or validation set, configurable)\n",
    "- Sample N examples (or use all, configurable)\n",
    "- Send them to multiple frontier models  \n",
    "- Score ROUGE  \n",
    "- Save predictions + latencies  \n",
    "- Produce qualitative examples  \n",
    "\n",
    "This establishes the **upper-bound performance baseline** for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef465dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101af05e",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import hashlib\n",
    "from src.utils.openrouter_client import ERROR_PREFIX\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c24c4c",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "**Important:** Set `RUN_API_CALLS` to `False` to load cached results instead of \n",
    "making new API calls. This saves money and time when you just want to analyze\n",
    "previous results.\n",
    "\n",
    "You can also control individual models with the `MODELS_TO_RUN` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c88466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# API CALL FLAGS - Set these to control what runs\n",
    "# =============================================================================\n",
    "\n",
    "# Master switch: Set False to load ALL results from cache (no API calls)\n",
    "RUN_API_CALLS = False\n",
    "\n",
    "# Per-model control: Set individual models to False to skip them\n",
    "# Only matters if RUN_API_CALLS = True\n",
    "MODELS_TO_RUN = {\n",
    "    \"gpt5_nano\":        True,\n",
    "    \"gpt5_mini\":        True,\n",
    "    \"gpt5_full\":        True,\n",
    "    \"gpt_oss_20b\":      True,\n",
    "    \"gpt_oss_120b\":     True,\n",
    "    \"gemini_25_flash\":  True,\n",
    "    \"claude_45_sonnet\": True,\n",
    "    \"qwen25_72b\":       True,\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# EXPERIMENT PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# EVALUATION MODE:\n",
    "# - \"test\": Use a small sample (fast, cheap) for debugging/development\n",
    "# - \"full\": Use all examples (slow, expensive) for final results\n",
    "EVALUATION_MODE = \"test\"  # Change to \"full\" when ready for final run\n",
    "\n",
    "# How many samples to use in \"test\" mode (ignored in \"full\" mode)\n",
    "TEST_MODE_SAMPLES = 100\n",
    "\n",
    "SEED = 42\n",
    "MAX_OUT_TOKENS = 512  # Max tokens for model responses\n",
    "\n",
    "# =============================================================================\n",
    "# WHICH DATASET SPLIT TO USE\n",
    "# =============================================================================\n",
    "USE_TEST_SET = True   # True = test set (for final comparison)\n",
    "                      # False = validation set (for development/debugging)\n",
    "\n",
    "# =============================================================================\n",
    "# PATHS\n",
    "# =============================================================================\n",
    "RESULTS_DIR = PROJECT_ROOT / \"experiments\" / \"exp3_api_llm_results\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ROUGE_SUMMARY_PATH = RESULTS_DIR / \"rouge_summary.csv\"\n",
    "LATENCY_SUMMARY_PATH = RESULTS_DIR / \"latency_summary.csv\"\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL DEFINITIONS\n",
    "# =============================================================================\n",
    "OPENROUTER_MODELS = {\n",
    "    # OpenAI family ‚Äì small ‚Üí big\n",
    "    \"gpt5_nano\":       \"openai/gpt-5-nano\",\n",
    "    \"gpt5_mini\":       \"openai/gpt-5-mini\",\n",
    "    \"gpt5_full\":       \"openai/gpt-5\",          # flagship upper bound\n",
    "\n",
    "    # OpenAI open-weight models\n",
    "    \"gpt_oss_20b\":     \"openai/gpt-oss-20b\",\n",
    "    \"gpt_oss_120b\":    \"openai/gpt-oss-120b\",\n",
    "\n",
    "    # Google Gemini ‚Äì fast, very strong general model\n",
    "    \"gemini_25_flash\": \"google/gemini-2.5-flash\",\n",
    "\n",
    "    # Anthropic Claude ‚Äì strong competitor\n",
    "    \"claude_45_sonnet\": \"anthropic/claude-4.5-sonnet-20250929\",\n",
    "\n",
    "    # Qwen ‚Äì top-tier open(-ish) model\n",
    "    \"qwen25_72b\":      \"qwen/qwen-2.5-72b-instruct\",\n",
    "}\n",
    "\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"RUN_API_CALLS: {RUN_API_CALLS}\")\n",
    "print(f\"Models configured: {len(OPENROUTER_MODELS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2927c5e",
   "metadata": {},
   "source": [
    "## 3. Load SAMSum Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.load_data import load_samsum\n",
    "\n",
    "train_df, val_df, test_df = load_samsum()\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Choose which dataset to evaluate on based on the config flag\n",
    "if USE_TEST_SET:\n",
    "    source_df = test_df\n",
    "    split_name = \"test\"\n",
    "    print(\"\\n‚úì Using TEST set for evaluation (for final comparison)\")\n",
    "else:\n",
    "    source_df = val_df\n",
    "    split_name = \"validation\"\n",
    "    print(\"\\n‚ö† Using VALIDATION set (for development only)\")\n",
    "\n",
    "SAMPLE_PATH = RESULTS_DIR / f\"evaluation_sample_{split_name}.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab4a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to reset the sample\n",
    "# if SAMPLE_PATH.exists():\n",
    "#     SAMPLE_PATH.unlink()\n",
    "#     print(\"Sample file deleted. Next run will create a new sample.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7129afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the evaluation set - BUT use consistent samples across runs\n",
    "\n",
    "def get_or_create_eval_sample(source_df, n_samples, seed, sample_path):\n",
    "    \"\"\"\n",
    "    Get evaluation sample with full validation of cached data.\n",
    "    \n",
    "    We save metadata alongside the sample to ensure we're using the right data:\n",
    "    - n_samples: Number of samples requested\n",
    "    - seed: Random seed used for sampling\n",
    "    - data_hash: Hash of the validation set to detect if source data changed\n",
    "    - source_df_len: Length of the original validation set\n",
    "    \"\"\"\n",
    "    metadata_path = sample_path.with_suffix('.meta.json')\n",
    "    \n",
    "    # Create a robust fingerprint of the source data\n",
    "    # We hash: the length, first 50 dialogues, last 50 dialogues, and some middle ones\n",
    "    # This catches changes anywhere in the dataset without hashing everything\n",
    "    source_len = len(source_df)\n",
    "    \n",
    "    # Create a fingerprint using:\n",
    "    # 1. Dataset length (catches additions/deletions)\n",
    "    # 2. Hash of ALL dialogue lengths (catches content changes anywhere)\n",
    "    # 3. First and last dialogue (catches obvious changes)\n",
    "    \n",
    "    dialogue_lengths = source_df['dialogue'].str.len().tolist()\n",
    "    \n",
    "    hash_data = {\n",
    "        \"length\": source_len,\n",
    "        \"dialogue_length_sum\": sum(dialogue_lengths),  # Fast check for any content change\n",
    "        \"dialogue_length_hash\": int(hashlib.md5(str(dialogue_lengths).encode()).hexdigest(), 16) % (10**10),\n",
    "        \"first_dialogue\": source_df['dialogue'].iloc[0][:200] if source_len > 0 else \"\",\n",
    "        \"last_dialogue\": source_df['dialogue'].iloc[-1][:200] if source_len > 0 else \"\",\n",
    "    }\n",
    "    \n",
    "    data_fingerprint = hashlib.md5(\n",
    "        json.dumps(hash_data, sort_keys=True).encode()\n",
    "    ).hexdigest()[:16]\n",
    "    \n",
    "    # Check if we have a valid cached sample\n",
    "    if sample_path.exists() and metadata_path.exists():\n",
    "        try:\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                saved_meta = json.load(f)\n",
    "            \n",
    "            # Validate ALL parameters match\n",
    "            if (saved_meta.get('n_samples') == n_samples and\n",
    "                saved_meta.get('seed') == seed and\n",
    "                saved_meta.get('data_hash') == data_fingerprint and\n",
    "                saved_meta.get('source_df_len') == source_len):  # Also check length\n",
    "                \n",
    "                existing_sample = pd.read_csv(sample_path)\n",
    "                \n",
    "                # Double-check row count matches\n",
    "                if len(existing_sample) == n_samples:\n",
    "                    print(f\"‚úì Loaded validated sample of {len(existing_sample)} dialogues\")\n",
    "                    print(f\"  (seed={seed}, source_len={source_len}, hash={data_fingerprint[:8]}...)\")\n",
    "                    return existing_sample\n",
    "                else:\n",
    "                    print(f\"‚ö† Sample file has wrong row count. Regenerating...\")\n",
    "            else:\n",
    "                # Explain what didn't match\n",
    "                mismatches = []\n",
    "                if saved_meta.get('n_samples') != n_samples:\n",
    "                    mismatches.append(f\"n_samples: {saved_meta.get('n_samples')} ‚Üí {n_samples}\")\n",
    "                if saved_meta.get('seed') != seed:\n",
    "                    mismatches.append(f\"seed: {saved_meta.get('seed')} ‚Üí {seed}\")\n",
    "                if saved_meta.get('source_df_len') != source_len:\n",
    "                    mismatches.append(f\"source_df_len: {saved_meta.get('source_df_len')} ‚Üí {source_len}\")\n",
    "                if saved_meta.get('data_hash') != data_fingerprint:\n",
    "                    mismatches.append(f\"data content changed\")\n",
    "                \n",
    "                print(f\"‚ö† Cached sample doesn't match current config:\")\n",
    "                for m in mismatches:\n",
    "                    print(f\"    - {m}\")\n",
    "                print(\"  Regenerating sample...\")\n",
    "                \n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"‚ö† Could not read metadata ({e}). Regenerating sample...\")\n",
    "    \n",
    "    # Create new sample\n",
    "    if n_samples >= len(source_df):\n",
    "        sample = source_df.copy().reset_index(drop=True)\n",
    "        print(f\"Using full validation set: {len(sample)} examples\")\n",
    "    else:\n",
    "        sample = source_df.sample(n=n_samples, random_state=seed).reset_index(drop=True)\n",
    "        print(f\"Created new sample of {len(sample)} examples (seed={seed})\")\n",
    "    \n",
    "    # Save sample and metadata\n",
    "    sample.to_csv(sample_path, index=False)\n",
    "    \n",
    "    metadata = {\n",
    "        'n_samples': n_samples,\n",
    "        'seed': seed,\n",
    "        'data_hash': data_fingerprint,\n",
    "        'source_df_len': source_len,  # Store original length\n",
    "        'created_at': pd.Timestamp.now().isoformat(),\n",
    "    }\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved sample to {sample_path.name}\")\n",
    "    print(f\"Saved metadata to {metadata_path.name}\")\n",
    "    \n",
    "    return sample\n",
    "\n",
    "# Determine how many samples to use based on evaluation mode\n",
    "if EVALUATION_MODE == \"full\":\n",
    "    N_SAMPLES = len(source_df)\n",
    "    print(f\"üìä FULL EVALUATION MODE: Using ALL {N_SAMPLES} examples\")\n",
    "    print(f\"   ‚ö†Ô∏è  This will take a while and cost more in API calls!\\n\")\n",
    "elif EVALUATION_MODE == \"test\":\n",
    "    N_SAMPLES = TEST_MODE_SAMPLES\n",
    "    print(f\"üß™ TEST MODE: Using {N_SAMPLES} examples (for development)\")\n",
    "    print(f\"   Change EVALUATION_MODE to 'full' for final results.\\n\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown EVALUATION_MODE: '{EVALUATION_MODE}'. Use 'test' or 'full'.\")\n",
    "\n",
    "eval_df = get_or_create_eval_sample(source_df, N_SAMPLES, SEED, SAMPLE_PATH)\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3585c52",
   "metadata": {},
   "source": [
    "## 4. Prompt Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15285819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summarization_prompt(dialogue: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a zero-shot summarization prompt for frontier LLMs.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"Summarize the following conversation in 1‚Äì2 sentences. \"\n",
    "        \"Keep it brief‚Äîaim for 15‚Äì30 words. \"\n",
    "        \"Focus on the main point, decisions, requests, or outcomes. \"\n",
    "        \"Ignore small talk and do not add details that aren't supported by the text.\\n\\n\"\n",
    "        \"DIALOGUE:\\n\"\n",
    "        \"-----\\n\"\n",
    "        f\"{dialogue}\\n\"\n",
    "        \"-----\\n\\n\"\n",
    "        \"SUMMARY:\"\n",
    "    )\n",
    "\n",
    "# Test the prompt\n",
    "print(\"Example prompt (truncated):\")\n",
    "print(build_summarization_prompt(eval_df['dialogue'].iloc[0])[:400] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2a04b",
   "metadata": {},
   "source": [
    "## 5. Run API Calls or Load Cached Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.logging import heading\n",
    "\n",
    "def get_cached_result_path(model_label: str) -> Path:\n",
    "    \"\"\"Get the path where results for a model would be cached.\"\"\"\n",
    "    return RESULTS_DIR / f\"{model_label}.csv\"\n",
    "\n",
    "\n",
    "def load_cached_results(model_label: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Load cached results for a model if they exist.\"\"\"\n",
    "    path = get_cached_result_path(model_label)\n",
    "    if path.exists():\n",
    "        return pd.read_csv(path)\n",
    "    return None\n",
    "\n",
    "\n",
    "def run_model_evaluation(model_label: str, model_id: str, eval_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Run API calls for a single model and return results DataFrame.\"\"\"\n",
    "    from tqdm.auto import tqdm\n",
    "    import src.utils.openrouter_client as openrouter_client\n",
    "    \n",
    "    rows = []\n",
    "    for i, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc=model_label):\n",
    "        dialogue = row[\"dialogue\"]\n",
    "        reference = row[\"summary\"]\n",
    "        prompt = build_summarization_prompt(dialogue)\n",
    "        \n",
    "        try:\n",
    "            pred, latency = openrouter_client.call_openrouter_llm(\n",
    "                model=model_id,\n",
    "                prompt=prompt,\n",
    "                max_tokens=MAX_OUT_TOKENS,\n",
    "                temperature=0.2,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            pred = f\"{ERROR_PREFIX} {e}]\"\n",
    "            latency = np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"dialogue\": dialogue,\n",
    "            \"reference_summary\": reference,\n",
    "            \"model_summary\": pred,\n",
    "            \"latency_seconds\": latency,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd6868",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"Loading/Running Model Evaluations\")\n",
    "\n",
    "results_by_model = {}\n",
    "models_run = []\n",
    "models_loaded = []\n",
    "models_skipped = []\n",
    "\n",
    "for label, model_id in OPENROUTER_MODELS.items():\n",
    "    cache_path = get_cached_result_path(label)\n",
    "    \n",
    "    if RUN_API_CALLS and MODELS_TO_RUN.get(label, True):\n",
    "        # Check if we should skip because results already exist\n",
    "        if cache_path.exists():\n",
    "            print(f\"\\n{label}: Cache exists. Re-running anyway (RUN_API_CALLS=True)\")\n",
    "        else:\n",
    "            print(f\"\\n{label}: Running API calls...\")\n",
    "        \n",
    "        # Run the evaluation\n",
    "        df_out = run_model_evaluation(label, model_id, eval_df)\n",
    "        \n",
    "        # Save to cache\n",
    "        df_out.to_csv(cache_path, index=False)\n",
    "        print(f\"  ‚úì Saved {len(df_out)} results to {cache_path.name}\")\n",
    "        \n",
    "        results_by_model[label] = df_out\n",
    "        models_run.append(label)\n",
    "        \n",
    "    else:\n",
    "        # Try to load from cache\n",
    "        cached = load_cached_results(label)\n",
    "        \n",
    "        if cached is not None:\n",
    "            results_by_model[label] = cached\n",
    "            models_loaded.append(label)\n",
    "            print(f\"{label}: Loaded {len(cached)} cached results\")\n",
    "        else:\n",
    "            models_skipped.append(label)\n",
    "            print(f\"{label}: No cached results found (skipping)\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY:\")\n",
    "print(f\"  Models run (API calls):  {len(models_run)} - {models_run}\")\n",
    "print(f\"  Models loaded (cache):   {len(models_loaded)} - {models_loaded}\")\n",
    "print(f\"  Models skipped:          {len(models_skipped)} - {models_skipped}\")\n",
    "print(f\"  Total models available:  {len(results_by_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9144c60",
   "metadata": {},
   "source": [
    "## 6. Compute ROUGE Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab5e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"ROUGE Evaluation\")\n",
    "\n",
    "from src.eval.rouge_eval import compute_rouge_from_df\n",
    "\n",
    "rouge_scores = {}\n",
    "sample_counts = {}\n",
    "\n",
    "for label, df in results_by_model.items():\n",
    "    valid_responses = df[~df[\"model_summary\"].str.startswith(ERROR_PREFIX)]\n",
    "    \n",
    "    if len(valid_responses) == 0:\n",
    "        print(f\"‚ùå {label}: No valid responses to evaluate (all failed)\")\n",
    "        continue\n",
    "    \n",
    "    error_count = len(df) - len(valid_responses)\n",
    "    if error_count > 0:\n",
    "        error_pct = error_count / len(df) * 100\n",
    "        print(f\"‚ö†Ô∏è  {label}: {error_count} errors ({error_pct:.1f}%), evaluating {len(valid_responses)} valid responses\")\n",
    "    \n",
    "    scores = compute_rouge_from_df(valid_responses)\n",
    "    rouge_scores[label] = scores\n",
    "    sample_counts[label] = len(valid_responses)\n",
    "\n",
    "# Create summary DataFrame\n",
    "if rouge_scores:\n",
    "    rouge_df = pd.DataFrame.from_dict(rouge_scores, orient=\"index\")\n",
    "    rouge_df.index.name = \"model\"\n",
    "    rouge_df[\"n_samples\"] = pd.Series(sample_counts)\n",
    "    rouge_df = rouge_df.sort_values(by=\"rougeL\", ascending=False)\n",
    "    \n",
    "    # Check for comparability issues\n",
    "    min_samples = rouge_df[\"n_samples\"].min()\n",
    "    max_samples = rouge_df[\"n_samples\"].max()\n",
    "\n",
    "    if min_samples != max_samples:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚ö†Ô∏è  COMPARABILITY WARNING\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Models were evaluated on different numbers of samples!\")\n",
    "        print(f\"  Smallest sample: {min_samples}\")\n",
    "        print(f\"  Largest sample:  {max_samples}\")\n",
    "        print(f\"\\nWhy this matters:\")\n",
    "        print(f\"  ‚Ä¢ Models with API errors were evaluated on fewer examples\")\n",
    "        print(f\"  ‚Ä¢ The failed examples are EXCLUDED from ROUGE calculation\")\n",
    "        print(f\"  ‚Ä¢ We don't know if failures correlate with difficulty\")\n",
    "        print(f\"  ‚Ä¢ This is SELECTION BIAS - scores may not be comparable\")\n",
    "        print(f\"\\nRecommendation:\")\n",
    "        print(f\"  ‚Ä¢ Re-run failed models to get complete results\")\n",
    "        print(f\"  ‚Ä¢ Or compare only on the intersection of successful examples\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    rouge_df.to_csv(ROUGE_SUMMARY_PATH)\n",
    "    print(f\"\\nSaved ROUGE summary to: {ROUGE_SUMMARY_PATH}\")\n",
    "    display(rouge_df)\n",
    "else:\n",
    "    rouge_df = None\n",
    "    print(\"No ROUGE scores to display.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5389090",
   "metadata": {},
   "source": [
    "## 7. Latency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4723cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"Latency Analysis\")\n",
    "\n",
    "latency_stats = {}\n",
    "\n",
    "for label, df in results_by_model.items():\n",
    "    vals = df[\"latency_seconds\"].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    if len(vals) == 0:\n",
    "        continue\n",
    "        \n",
    "    desc = vals.describe(percentiles=[0.5, 0.9, 0.95])\n",
    "    latency_stats[label] = desc\n",
    "\n",
    "if latency_stats:\n",
    "    latency_df = pd.DataFrame(latency_stats).T\n",
    "    latency_df.index.name = \"model\"\n",
    "    \n",
    "    # Prettier column names\n",
    "    latency_df = latency_df.rename(\n",
    "        columns={\n",
    "            \"count\": \"count\",\n",
    "            \"mean\": \"mean\",\n",
    "            \"std\": \"std\",\n",
    "            \"min\": \"min\",\n",
    "            \"50%\": \"p50\",\n",
    "            \"90%\": \"p90\",\n",
    "            \"95%\": \"p95\",\n",
    "            \"max\": \"max\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Sort by mean latency (fastest first)\n",
    "    latency_df = latency_df.sort_values(by=\"mean\", ascending=True)\n",
    "    \n",
    "    # Save summary\n",
    "    latency_df.to_csv(LATENCY_SUMMARY_PATH)\n",
    "    print(f\"Saved latency summary to: {LATENCY_SUMMARY_PATH}\")\n",
    "    \n",
    "    display(latency_df)\n",
    "else:\n",
    "    latency_df = None\n",
    "    print(\"No latency data to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a81b0",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rouge_df is not None and len(rouge_df) > 0:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # ROUGE scores bar chart\n",
    "    ax = axes[0]\n",
    "    rouge_plot_df = rouge_df[[\"rouge1\", \"rouge2\", \"rougeL\"]].sort_values(\"rougeL\", ascending=True)\n",
    "    rouge_plot_df.plot(kind=\"barh\", ax=ax)\n",
    "    ax.set_xlabel(\"Score\")\n",
    "    ax.set_title(\"ROUGE Scores by Model\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "    \n",
    "    # Latency bar chart\n",
    "    ax = axes[1]\n",
    "    if latency_df is not None and len(latency_df) > 0:\n",
    "        latency_plot = latency_df[\"mean\"].sort_values(ascending=True)\n",
    "        latency_plot.plot(kind=\"barh\", ax=ax, color=\"coral\")\n",
    "        ax.set_xlabel(\"Mean Latency (seconds)\")\n",
    "        ax.set_title(\"API Latency by Model\")\n",
    "        ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"No latency data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "        ax.set_title(\"API Latency by Model\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / \"rouge_latency_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved figure to: {RESULTS_DIR / 'rouge_latency_comparison.png'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c324066f",
   "metadata": {},
   "source": [
    "## 9. Qualitative Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0631b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"Qualitative Examples\")\n",
    "\n",
    "def show_examples(df, model_label, n=5, max_chars=600):\n",
    "    \"\"\"Show n random examples from a model's results.\"\"\"\n",
    "    # Filter out errors\n",
    "    valid_df = df[~df[\"model_summary\"].str.startswith(ERROR_PREFIX)]\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        print(f\"No valid examples for {model_label}\")\n",
    "        return\n",
    "    \n",
    "    sample = valid_df.sample(n=min(n, len(valid_df)), random_state=SEED)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Examples for: {model_label}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for _, row in sample.iterrows():\n",
    "        d = row[\"dialogue\"]\n",
    "        d = d[:max_chars] + \" ... [truncated]\" if len(d) > max_chars else d\n",
    "        \n",
    "        print(\"\\n--- Example ---\")\n",
    "        print(f\"[DIALOGUE]\\n{d}\")\n",
    "        print(f\"\\n[HUMAN SUMMARY]\\n{row['reference_summary']}\")\n",
    "        print(f\"\\n[MODEL SUMMARY]\\n{row['model_summary']}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# Show examples for each model\n",
    "for label, df in results_by_model.items():\n",
    "    show_examples(df, label, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903bb9b1",
   "metadata": {},
   "source": [
    "## 10. Side-by-Side Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"Side-by-Side Comparison (Same Examples)\")\n",
    "\n",
    "# Check we have enough models to compare\n",
    "if len(results_by_model) < 2:\n",
    "    print(\"Need at least 2 models for side-by-side comparison.\")\n",
    "    print(f\"Currently have: {len(results_by_model)} model(s)\")\n",
    "else:\n",
    "    # Get dialogues from each model\n",
    "    dialogue_sets = {\n",
    "        label: set(df[\"dialogue\"].tolist()) \n",
    "        for label, df in results_by_model.items()\n",
    "    }\n",
    "    \n",
    "    # Safely compute intersection\n",
    "    all_dialogue_sets = list(dialogue_sets.values())\n",
    "    if len(all_dialogue_sets) == 0:\n",
    "        print(\"‚ùå No dialogue sets to compare!\")\n",
    "        common_dialogues = set()\n",
    "    elif len(all_dialogue_sets) == 1:\n",
    "        common_dialogues = all_dialogue_sets[0]\n",
    "    else:\n",
    "        # Start with first set, intersect with all others\n",
    "        common_dialogues = all_dialogue_sets[0].copy()\n",
    "        for other_set in all_dialogue_sets[1:]:\n",
    "            common_dialogues &= other_set  # Same as intersection\n",
    "    \n",
    "    if len(common_dialogues) == 0:\n",
    "        print(\"‚ùå No common dialogues found across all models!\")\n",
    "        print(\"   This can happen if models were evaluated on different samples.\")\n",
    "        print(\"\\n   Dialogues per model:\")\n",
    "        for label, dialogues in dialogue_sets.items():\n",
    "            print(f\"     {label}: {len(dialogues)} dialogues\")\n",
    "    else:\n",
    "        print(f\"Found {len(common_dialogues)} dialogues common to all {len(results_by_model)} models.\")\n",
    "        \n",
    "        # Sample from common dialogues only\n",
    "        common_list = list(common_dialogues)\n",
    "        n_examples = min(3, len(common_list))\n",
    "        \n",
    "        # Use numpy for reproducible sampling from a list\n",
    "        rng = np.random.default_rng(SEED)\n",
    "        sample_indices = rng.choice(len(common_list), size=n_examples, replace=False)\n",
    "        sample_dialogues = [common_list[i] for i in sample_indices]\n",
    "        \n",
    "        # Get references from the first model's dataframe\n",
    "        first_model = list(results_by_model.keys())[0]\n",
    "        first_df = results_by_model[first_model]\n",
    "        \n",
    "        for i, dialogue in enumerate(sample_dialogues):\n",
    "            # Get reference from first model's data\n",
    "            ref_row = first_df[first_df[\"dialogue\"] == dialogue].iloc[0]\n",
    "            reference = ref_row[\"reference_summary\"]\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"EXAMPLE {i+1}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            print(f\"\\n[DIALOGUE]\\n{dialogue[:500]}{'...' if len(dialogue) > 500 else ''}\")\n",
    "            print(f\"\\n[HUMAN SUMMARY]\\n{reference}\")\n",
    "            print(f\"\\n[MODEL SUMMARIES]\")\n",
    "            \n",
    "            # Show each model's summary for this dialogue\n",
    "            for label, df in results_by_model.items():\n",
    "                matching_rows = df[df[\"dialogue\"] == dialogue]\n",
    "                \n",
    "                if len(matching_rows) > 0:\n",
    "                    model_summary = matching_rows.iloc[0][\"model_summary\"]\n",
    "                    # Truncate long summaries for display\n",
    "                    if len(model_summary) > 200:\n",
    "                        model_summary = model_summary[:200] + \"...\"\n",
    "                    print(f\"  {label:20s}: {model_summary}\")\n",
    "                else:\n",
    "                    # This shouldn't happen now, but keep as safety\n",
    "                    print(f\"  {label:20s}: [NOT FOUND]\")\n",
    "            \n",
    "            print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a13f8de",
   "metadata": {},
   "source": [
    "## 11. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e2140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"Error Analysis\")\n",
    "\n",
    "error_summary = []\n",
    "\n",
    "for label, df in results_by_model.items():\n",
    "    total = len(df)\n",
    "    errors = df[\"model_summary\"].str.startswith(ERROR_PREFIX).sum()\n",
    "    error_rate = errors / total * 100 if total > 0 else 0\n",
    "    \n",
    "    error_summary.append({\n",
    "        \"model\": label,\n",
    "        \"total_requests\": total,\n",
    "        \"errors\": errors,\n",
    "        \"error_rate_pct\": f\"{error_rate:.1f}%\",\n",
    "    })\n",
    "    \n",
    "    # Show sample errors if any\n",
    "    if errors > 0:\n",
    "        error_samples = df[df[\"model_summary\"].str.startswith(ERROR_PREFIX)][\"model_summary\"].head(2).tolist()\n",
    "        print(f\"{label}: {errors} errors ({error_rate:.1f}%)\")\n",
    "        for err in error_samples:\n",
    "            print(f\"  - {err[:100]}...\")\n",
    "\n",
    "error_df = pd.DataFrame(error_summary)\n",
    "display(error_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f11749",
   "metadata": {},
   "source": [
    "## 12. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edce606",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading(\"Final Summary\")\n",
    "\n",
    "if rouge_df is not None and latency_df is not None:\n",
    "    # Combine ROUGE and latency into one summary\n",
    "    summary_data = []\n",
    "    \n",
    "    # Check if sample counts vary\n",
    "    min_samples = rouge_df[\"n_samples\"].min()\n",
    "    max_samples = rouge_df[\"n_samples\"].max()\n",
    "    samples_vary = min_samples != max_samples\n",
    "    \n",
    "    for model in rouge_df.index:\n",
    "        row = {\n",
    "            \"Model\": model,\n",
    "            \"N\": int(rouge_df.loc[model, \"n_samples\"]),\n",
    "            \"ROUGE-1\": f\"{rouge_df.loc[model, 'rouge1']:.3f}\",\n",
    "            \"ROUGE-2\": f\"{rouge_df.loc[model, 'rouge2']:.3f}\",\n",
    "            \"ROUGE-L\": f\"{rouge_df.loc[model, 'rougeL']:.3f}\",\n",
    "        }\n",
    "        \n",
    "        if model in latency_df.index:\n",
    "            row[\"Mean Latency (s)\"] = f\"{latency_df.loc[model, 'mean']:.2f}\"\n",
    "            row[\"P95 Latency (s)\"] = f\"{latency_df.loc[model, 'p95']:.2f}\"\n",
    "        else:\n",
    "            row[\"Mean Latency (s)\"] = \"N/A\"\n",
    "            row[\"P95 Latency (s)\"] = \"N/A\"\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    final_summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Only sort and present rankings if samples are comparable\n",
    "    # =========================================================================\n",
    "    if samples_vary:\n",
    "        print(\"‚ö†Ô∏è  WARNING: Models were evaluated on different sample sizes!\")\n",
    "        print(f\"   Sample counts range from {min_samples} to {max_samples}.\")\n",
    "        print(\"   ROUGE scores may NOT be directly comparable.\")\n",
    "        print(\"   Models with fewer samples had more API errors.\\n\")\n",
    "        print(\"   Showing results sorted by model name (NOT by score):\\n\")\n",
    "        final_summary_df = final_summary_df.sort_values(\"Model\")\n",
    "    else:\n",
    "        print(f\"‚úì All models evaluated on {max_samples} samples. Rankings are valid.\\n\")\n",
    "        final_summary_df = final_summary_df.sort_values(\"ROUGE-L\", ascending=False)\n",
    "    \n",
    "    final_summary_df = final_summary_df.reset_index(drop=True)\n",
    "    \n",
    "    # Save final summary\n",
    "    final_summary_path = RESULTS_DIR / \"final_summary.csv\"\n",
    "    final_summary_df.to_csv(final_summary_path, index=False)\n",
    "    print(f\"Saved final summary to: {final_summary_path}\")\n",
    "    \n",
    "    display(final_summary_df)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # NEW: Save metadata about this evaluation run\n",
    "    # This helps notebook 05 know which split was used\n",
    "    # =========================================================================\n",
    "    \n",
    "    eval_metadata = {\n",
    "        \"split_used\": split_name,\n",
    "        \"n_samples\": len(eval_df),\n",
    "        \"seed\": SEED,\n",
    "        \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "    }\n",
    "    \n",
    "    metadata_path = RESULTS_DIR / \"evaluation_metadata.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(eval_metadata, f, indent=2)\n",
    "    print(f\"Saved evaluation metadata to: {metadata_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Insufficient data for final summary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc290f3",
   "metadata": {},
   "source": [
    "---\n",
    "# 13. Key Takeaways ‚Äî Experiment 3 (Frontier LLMs via OpenRouter)\n",
    "---\n",
    "\n",
    "*Fill this in after running:*\n",
    "\n",
    "**ROUGE Performance:**\n",
    "- Best performing model: [TBD]\n",
    "- Worst performing model: [TBD]\n",
    "- Score range: [TBD]\n",
    "\n",
    "**Latency:**\n",
    "- Fastest model: [TBD]\n",
    "- Slowest model: [TBD]\n",
    "- Latency range: [TBD]\n",
    "\n",
    "**Quality Observations:**\n",
    "- Style differences (concise vs narrative): [TBD]\n",
    "- Error cases or hallucinations: [TBD]\n",
    "- Which model follows instructions best: [TBD]\n",
    "\n",
    "**Cost/Latency Trade-offs:**\n",
    "- Best value (quality/latency ratio): [TBD]\n",
    "- Comparison to local fine-tuned models: [TBD]\n",
    "\n",
    "**Recommendation for Final Comparison:**\n",
    "- Model to use as \"frontier baseline\": [TBD]\n",
    "- Reasoning: [TBD]\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
