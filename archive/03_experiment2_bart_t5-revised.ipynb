{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# ![Banner](https://github.com/LittleHouse75/flatiron-resources/raw/main/NevitsBanner.png)\n",
    "---\n",
    "# Experiment 2 — BART & T5 (Pretrained Seq2Seq Models)\n",
    "### Purpose-Built Encoder-Decoder Summarization\n",
    "---\n",
    "\n",
    "This notebook implements **Experiment 2** for the capstone project:\n",
    "\n",
    "**Goal:**  \n",
    "Evaluate purpose-built sequence-to-sequence models for dialogue summarization:\n",
    "\n",
    "- **BART** (`facebook/bart-base`) — Denoising autoencoder pretrained for seq2seq tasks\n",
    "- **T5** (`t5-small`) — Text-to-text transformer pretrained on C4\n",
    "\n",
    "Unlike Experiment 1's \"Frankenstein\" BERT→GPT-2, these models have **pretrained cross-attention**\n",
    "layers, meaning the encoder and decoder already know how to communicate.\n",
    "\n",
    "**What This Notebook Covers:**\n",
    "1. Model construction and tokenizer setup\n",
    "2. Fine-tuning on SAMSum using `Seq2SeqTrainer`\n",
    "3. Evaluation on both validation and **test sets**\n",
    "4. ROUGE metrics and qualitative analysis\n",
    "5. Side-by-side comparison of BART vs T5\n",
    "\n",
    "**Note:** This notebook parallels the structure of `02_experiment1_bert_gpt2-revised.ipynb`\n",
    "for consistency across experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Mute common warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*requires_grad.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Project root for imports\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "All hyperparameters and paths in one place for easy modification.\n",
    "\n",
    "**Note:** Parameters are aligned with Experiment 1 for fair comparison,\n",
    "with adjustments where appropriate for pretrained seq2seq models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING FLAGS\n",
    "# =============================================================================\n",
    "RUN_TRAINING_BART = True  # Set False to load from checkpoint\n",
    "RUN_TRAINING_T5 = True    # Set False to load from checkpoint\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# =============================================================================\n",
    "BART_MODEL_NAME = \"facebook/bart-base\"\n",
    "T5_MODEL_NAME = \"t5-small\"\n",
    "T5_PREFIX = \"summarize: \"  # Required prefix for T5\n",
    "\n",
    "# =============================================================================\n",
    "# SEQUENCE LENGTHS\n",
    "# =============================================================================\n",
    "MAX_SOURCE_LEN = 512  # Dialogue input length\n",
    "MAX_TARGET_LEN = 128  # Summary output length\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING HYPERPARAMETERS\n",
    "# (Aligned with Experiment 1 for fair comparison)\n",
    "# =============================================================================\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM_STEPS = 2        # Effective batch size = 4 * 2 = 8\n",
    "NUM_EPOCHS = 6              # BART/T5 converge faster than BERT→GPT2\n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_STEPS = 500\n",
    "WEIGHT_DECAY = 0.01\n",
    "LOGGING_STEPS = 100\n",
    "\n",
    "# Early stopping\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "\n",
    "# Generation settings\n",
    "NUM_BEAMS = 4\n",
    "NO_REPEAT_NGRAM_SIZE = 3\n",
    "LENGTH_PENALTY = 2.0\n",
    "MIN_LENGTH = 5\n",
    "\n",
    "# =============================================================================\n",
    "# PATHS\n",
    "# =============================================================================\n",
    "# BART paths\n",
    "BART_OUTPUT_DIR = PROJECT_ROOT / \"models\" / \"bart\"\n",
    "BART_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BART_CHECKPOINT_DIR = BART_OUTPUT_DIR / \"checkpoints\"\n",
    "BART_BEST_DIR = BART_OUTPUT_DIR / \"best\"\n",
    "BART_HISTORY_PATH = BART_OUTPUT_DIR / \"history.csv\"\n",
    "BART_TEST_RESULTS_PATH = BART_OUTPUT_DIR / \"test_results.csv\"\n",
    "\n",
    "# T5 paths\n",
    "T5_OUTPUT_DIR = PROJECT_ROOT / \"models\" / \"t5\"\n",
    "T5_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "T5_CHECKPOINT_DIR = T5_OUTPUT_DIR / \"checkpoints\"\n",
    "T5_BEST_DIR = T5_OUTPUT_DIR / \"best\"\n",
    "T5_HISTORY_PATH = T5_OUTPUT_DIR / \"history.csv\"\n",
    "T5_TEST_RESULTS_PATH = T5_OUTPUT_DIR / \"test_results.csv\"\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"  BART model: {BART_MODEL_NAME}\")\n",
    "print(f\"  T5 model: {T5_MODEL_NAME}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS}\")\n",
    "print(f\"  BART output: {BART_OUTPUT_DIR}\")\n",
    "print(f\"  T5 output: {T5_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 3. Load SAMSum Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-load-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.load_data import load_samsum\n",
    "\n",
    "train_df, val_df, test_df = load_samsum()\n",
    "\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"  Train:      {len(train_df):,} examples\")\n",
    "print(f\"  Validation: {len(val_df):,} examples\")\n",
    "print(f\"  Test:       {len(test_df):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-sample-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick peek at the data\n",
    "print(\"Sample dialogue:\")\n",
    "print(\"-\" * 40)\n",
    "print(train_df.iloc[0][\"dialogue\"][:300], \"...\")\n",
    "print()\n",
    "print(\"Sample summary:\")\n",
    "print(\"-\" * 40)\n",
    "print(train_df.iloc[0][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-imports-header",
   "metadata": {},
   "source": [
    "## 4. Shared Imports and Utilities\n",
    "\n",
    "Import common components used by both BART and T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# Load ROUGE metric (shared by both models)\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    \"\"\"\n",
    "    Compute ROUGE scores for evaluation.\n",
    "    This function is parameterized by tokenizer to work with both BART and T5.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Replace -100 with pad token id for decoding\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up whitespace\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE\n",
    "    result = rouge_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "    \n",
    "    # Convert to percentages for readability\n",
    "    result = {k: round(v * 100, 2) for k, v in result.items()}\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Shared utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bart-section-header",
   "metadata": {},
   "source": [
    "---\n",
    "# PART A: BART\n",
    "---\n",
    "\n",
    "BART (Bidirectional and Auto-Regressive Transformers) is a denoising autoencoder\n",
    "pretrained by corrupting text and learning to reconstruct it. This makes it\n",
    "naturally suited for seq2seq tasks like summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bart-tokenizer-header",
   "metadata": {},
   "source": [
    "## 5A. BART Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-tokenizer-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(BART_MODEL_NAME)\n",
    "\n",
    "print(f\"BART Tokenizer: {bart_tokenizer.__class__.__name__}\")\n",
    "print(f\"  Vocab size: {len(bart_tokenizer):,}\")\n",
    "print(f\"  Pad token: '{bart_tokenizer.pad_token}' (id: {bart_tokenizer.pad_token_id})\")\n",
    "print(f\"  BOS token: '{bart_tokenizer.bos_token}' (id: {bart_tokenizer.bos_token_id})\")\n",
    "print(f\"  EOS token: '{bart_tokenizer.eos_token}' (id: {bart_tokenizer.eos_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bart-model-header",
   "metadata": {},
   "source": [
    "## 6A. Build or Load BART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-model-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "if RUN_TRAINING_BART:\n",
    "    print(\"Building fresh BART model for training...\")\n",
    "    \n",
    "    bart_model = BartForConditionalGeneration.from_pretrained(BART_MODEL_NAME)\n",
    "    \n",
    "    # Configure generation defaults\n",
    "    bart_model.config.max_length = MAX_TARGET_LEN\n",
    "    bart_model.config.min_length = MIN_LENGTH\n",
    "    bart_model.config.num_beams = NUM_BEAMS\n",
    "    bart_model.config.no_repeat_ngram_size = NO_REPEAT_NGRAM_SIZE\n",
    "    bart_model.config.length_penalty = LENGTH_PENALTY\n",
    "    bart_model.config.early_stopping = True\n",
    "    \n",
    "    print(f\"\\nBART model built successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Loading BART model from checkpoint: {BART_BEST_DIR}\")\n",
    "    bart_model = BartForConditionalGeneration.from_pretrained(BART_BEST_DIR)\n",
    "    bart_tokenizer = BartTokenizer.from_pretrained(BART_BEST_DIR)\n",
    "    print(\"BART model loaded successfully!\")\n",
    "\n",
    "# Move to device\n",
    "bart_model = bart_model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in bart_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in bart_model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bart-dataset-header",
   "metadata": {},
   "source": [
    "## 7A. Prepare BART Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-dataset-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_bart(examples):\n",
    "    \"\"\"\n",
    "    Tokenize dialogues and summaries for BART.\n",
    "    BART doesn't need a task prefix.\n",
    "    \"\"\"\n",
    "    # Encode dialogues\n",
    "    model_inputs = bart_tokenizer(\n",
    "        examples[\"dialogue\"],\n",
    "        max_length=MAX_SOURCE_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # Encode summaries (as labels)\n",
    "    labels = bart_tokenizer(\n",
    "        text_target=examples[\"summary\"],\n",
    "        max_length=MAX_TARGET_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # Replace padding token id with -100 for loss calculation\n",
    "    labels_ids = np.array(labels[\"input_ids\"])\n",
    "    labels_ids[labels_ids == bart_tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels_ids.tolist()\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Convert DataFrames to HuggingFace Datasets\n",
    "print(\"Converting to HuggingFace Datasets...\")\n",
    "bart_train_dataset = Dataset.from_pandas(train_df[[\"dialogue\", \"summary\"]])\n",
    "bart_val_dataset = Dataset.from_pandas(val_df[[\"dialogue\", \"summary\"]])\n",
    "bart_test_dataset = Dataset.from_pandas(test_df[[\"dialogue\", \"summary\"]])\n",
    "\n",
    "# Tokenize\n",
    "print(\"Tokenizing datasets for BART...\")\n",
    "bart_tokenized_train = bart_train_dataset.map(\n",
    "    preprocess_bart,\n",
    "    batched=True,\n",
    "    remove_columns=[\"dialogue\", \"summary\"],\n",
    "    desc=\"Tokenizing train\",\n",
    ")\n",
    "\n",
    "bart_tokenized_val = bart_val_dataset.map(\n",
    "    preprocess_bart,\n",
    "    batched=True,\n",
    "    remove_columns=[\"dialogue\", \"summary\"],\n",
    "    desc=\"Tokenizing validation\",\n",
    ")\n",
    "\n",
    "bart_tokenized_test = bart_test_dataset.map(\n",
    "    preprocess_bart,\n",
    "    batched=True,\n",
    "    remove_columns=[\"dialogue\", \"summary\"],\n",
    "    desc=\"Tokenizing test\",\n",
    ")\n",
    "\n",
    "print(f\"\\nBART tokenized datasets:\")\n",
    "print(f\"  Train: {len(bart_tokenized_train):,} examples\")\n",
    "print(f\"  Validation: {len(bart_tokenized_val):,} examples\")\n",
    "print(f\"  Test: {len(bart_tokenized_test):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bart-collator-header",
   "metadata": {},
   "source": [
    "## 8A. BART Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-collator-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=bart_tokenizer,\n",
    "    model=bart_model,\n",
    "    label_pad_token_id=-100,\n",
    ")\n",
    "\n",
    "# Create metrics function with BART tokenizer\n",
    "def bart_compute_metrics(eval_pred):\n",
    "    return compute_metrics(eval_pred, bart_tokenizer)\n",
    "\n",
    "print(\"BART data collator configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bart-training-header",
   "metadata": {},
   "source": [
    "## 9A. Train BART with Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-trainer-setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TRAINING_BART:\n",
    "    print(\"Setting up BART Seq2SeqTrainer...\")\n",
    "    print(f\"\\nTraining configuration:\")\n",
    "    print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"  Gradient accumulation: {GRAD_ACCUM_STEPS}\")\n",
    "    print(f\"  Effective batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS}\")\n",
    "    print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"  Warmup steps: {WARMUP_STEPS}\")\n",
    "    print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "    \n",
    "    bart_training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(BART_CHECKPOINT_DIR),\n",
    "        \n",
    "        # Training\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "        \n",
    "        # Optimization\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        \n",
    "        # Evaluation & Saving\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rougeL\",  # Use ROUGE-L, not val_loss!\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        \n",
    "        # Generation during evaluation\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=MAX_TARGET_LEN,\n",
    "        generation_num_beams=NUM_BEAMS,\n",
    "        \n",
    "        # Logging\n",
    "        logging_dir=str(BART_OUTPUT_DIR / \"logs\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        \n",
    "        # Performance\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=0,\n",
    "        \n",
    "        # Reproducibility\n",
    "        seed=SEED,\n",
    "    )\n",
    "    \n",
    "    bart_trainer = Seq2SeqTrainer(\n",
    "        model=bart_model,\n",
    "        args=bart_training_args,\n",
    "        train_dataset=bart_tokenized_train,\n",
    "        eval_dataset=bart_tokenized_val,\n",
    "        tokenizer=bart_tokenizer,\n",
    "        data_collator=bart_data_collator,\n",
    "        compute_metrics=bart_compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(\n",
    "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "            early_stopping_threshold=0.0,\n",
    "        )],\n",
    "    )\n",
    "    \n",
    "    print(\"\\nBART Trainer configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-training-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TRAINING_BART:\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING BART TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    bart_train_result = bart_trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BART TRAINING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nTraining time: {bart_train_result.metrics['train_runtime']:.1f} seconds\")\n",
    "    print(f\"Final training loss: {bart_train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bart-save-header",
   "metadata": {},
   "source": [
    "## 10A. Save BART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-save-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TRAINING_BART:\n",
    "    print(f\"\\nSaving best BART model to: {BART_BEST_DIR}\")\n",
    "    bart_trainer.save_model(str(BART_BEST_DIR))\n",
    "    bart_tokenizer.save_pretrained(BART_BEST_DIR)\n",
    "    bart_trainer.save_state()\n",
    "    print(\"BART model and tokenizer saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bart-history-header",
   "metadata": {},
   "source": [
    "## 11A. BART Training History & Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-history-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TRAINING_BART:\n",
    "    # Extract training history from trainer state\n",
    "    bart_log_history = bart_trainer.state.log_history\n",
    "    \n",
    "    # Separate evaluation logs\n",
    "    bart_eval_logs = [log for log in bart_log_history if \"eval_loss\" in log]\n",
    "    \n",
    "    # Create history DataFrame\n",
    "    bart_history_data = []\n",
    "    for eval_log in bart_eval_logs:\n",
    "        epoch = eval_log.get(\"epoch\", 0)\n",
    "        bart_history_data.append({\n",
    "            \"epoch\": int(epoch),\n",
    "            \"train_loss\": eval_log.get(\"train_loss\", np.nan),\n",
    "            \"val_loss\": eval_log.get(\"eval_loss\", np.nan),\n",
    "            \"rouge1\": eval_log.get(\"eval_rouge1\", np.nan),\n",
    "            \"rouge2\": eval_log.get(\"eval_rouge2\", np.nan),\n",
    "            \"rougeL\": eval_log.get(\"eval_rougeL\", np.nan),\n",
    "            \"rougeLsum\": eval_log.get(\"eval_rougeLsum\", np.nan),\n",
    "        })\n",
    "    \n",
    "    bart_history_df = pd.DataFrame(bart_history_data)\n",
    "    bart_history_df.to_csv(BART_HISTORY_PATH, index=False)\n",
    "    print(f\"BART training history saved to: {BART_HISTORY_PATH}\")\n",
    "    \n",
    "else:\n",
    "    if BART_HISTORY_PATH.exists():\n",
    "        bart_history_df = pd.read_csv(BART_HISTORY_PATH)\n",
    "        print(f\"Loaded BART training history from: {BART_HISTORY_PATH}\")\n",
    "    else:\n",
    "        bart_history_df = None\n",
    "        print(\"No BART training history found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-history-display-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bart_history_df is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BART TRAINING HISTORY\")\n",
    "    print(\"=\"*60)\n",
    "    display(bart_history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-curves-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if bart_history_df is not None and len(bart_history_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax = axes[0]\n",
    "    if \"train_loss\" in bart_history_df.columns:\n",
    "        ax.plot(bart_history_df[\"epoch\"], bart_history_df[\"train_loss\"], marker=\"o\", label=\"Train Loss\")\n",
    "    ax.plot(bart_history_df[\"epoch\"], bart_history_df[\"val_loss\"], marker=\"o\", label=\"Val Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"BART — Loss Curves\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ROUGE curves\n",
    "    ax = axes[1]\n",
    "    ax.plot(bart_history_df[\"epoch\"], bart_history_df[\"rouge1\"], marker=\"o\", label=\"ROUGE-1\")\n",
    "    ax.plot(bart_history_df[\"epoch\"], bart_history_df[\"rouge2\"], marker=\"o\", label=\"ROUGE-2\")\n",
    "    ax.plot(bart_history_df[\"epoch\"], bart_history_df[\"rougeL\"], marker=\"o\", label=\"ROUGE-L\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"ROUGE Score\")\n",
    "    ax.set_title(\"BART — ROUGE Scores (Validation)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_path = BART_OUTPUT_DIR / \"training_curves.png\"\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"Saved BART training curves to: {fig_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bart-val-qual-header",
   "metadata": {},
   "source": [
    "## 12A. BART Validation Qualitative Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-val-qual-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.qualitative import qualitative_samples\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BART VALIDATION SET: Qualitative Examples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "qualitative_samples(\n",
    "    df=val_df,\n",
    "    model=bart_model,\n",
    "    encoder_tokenizer=bart_tokenizer,\n",
    "    decoder_tokenizer=bart_tokenizer,\n",
    "    device=device,\n",
    "    max_source_len=MAX_SOURCE_LEN,\n",
    "    max_target_len=MAX_TARGET_LEN,\n",
    "    source_prefix=\"\",  # No prefix for BART\n",
    "    n=5,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bart-test-header",
   "metadata": {},
   "source": [
    "## 13A. BART Test Set Evaluation\n",
    "\n",
    "Evaluate on the **held-out test set** — the final measure of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-test-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BART TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if RUN_TRAINING_BART:\n",
    "    print(\"\\nRunning BART evaluation on test set...\")\n",
    "    bart_test_results = bart_trainer.evaluate(eval_dataset=bart_tokenized_test)\n",
    "else:\n",
    "    # Create a trainer just for evaluation\n",
    "    print(\"\\nCreating BART trainer for evaluation...\")\n",
    "    \n",
    "    bart_eval_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(BART_OUTPUT_DIR / \"eval_temp\"),\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=MAX_TARGET_LEN,\n",
    "        generation_num_beams=NUM_BEAMS,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "    \n",
    "    bart_eval_trainer = Seq2SeqTrainer(\n",
    "        model=bart_model,\n",
    "        args=bart_eval_args,\n",
    "        tokenizer=bart_tokenizer,\n",
    "        data_collator=bart_data_collator,\n",
    "        compute_metrics=bart_compute_metrics,\n",
    "    )\n",
    "    \n",
    "    print(\"Running BART evaluation on test set...\")\n",
    "    bart_test_results = bart_eval_trainer.evaluate(eval_dataset=bart_tokenized_test)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"BART TEST SET RESULTS\")\n",
    "print(\"-\"*40)\n",
    "print(f\"  Loss:      {bart_test_results['eval_loss']:.4f}\")\n",
    "print(f\"  ROUGE-1:   {bart_test_results['eval_rouge1']:.2f}\")\n",
    "print(f\"  ROUGE-2:   {bart_test_results['eval_rouge2']:.2f}\")\n",
    "print(f\"  ROUGE-L:   {bart_test_results['eval_rougeL']:.2f}\")\n",
    "print(f\"  ROUGE-Lsum:{bart_test_results['eval_rougeLsum']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-test-save-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save BART test results\n",
    "bart_test_results_df = pd.DataFrame([{\n",
    "    \"model\": \"BART\",\n",
    "    \"test_loss\": bart_test_results[\"eval_loss\"],\n",
    "    \"rouge1\": bart_test_results[\"eval_rouge1\"],\n",
    "    \"rouge2\": bart_test_results[\"eval_rouge2\"],\n",
    "    \"rougeL\": bart_test_results[\"eval_rougeL\"],\n",
    "    \"rougeLsum\": bart_test_results[\"eval_rougeLsum\"],\n",
    "}])\n",
    "\n",
    "bart_test_results_df.to_csv(BART_TEST_RESULTS_PATH, index=False)\n",
    "print(f\"\\nBART test results saved to: {BART_TEST_RESULTS_PATH}\")\n",
    "display(bart_test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bart-test-qual-header",
   "metadata": {},
   "source": [
    "## 14A. BART Test Set Qualitative Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-test-qual-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BART TEST SET: Qualitative Examples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "qualitative_samples(\n",
    "    df=test_df,\n",
    "    model=bart_model,\n",
    "    encoder_tokenizer=bart_tokenizer,\n",
    "    decoder_tokenizer=bart_tokenizer,\n",
    "    device=device,\n",
    "    max_source_len=MAX_SOURCE_LEN,\n",
    "    max_target_len=MAX_TARGET_LEN,\n",
    "    source_prefix=\"\",\n",
    "    n=5,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bart-predictions-header",
   "metadata": {},
   "source": [
    "## 15A. Generate All BART Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-predictions-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from src.eval.qualitative import generate_summary\n",
    "\n",
    "print(\"\\nGenerating BART predictions for all test examples...\")\n",
    "print(\"This may take a few minutes.\\n\")\n",
    "\n",
    "bart_test_predictions = []\n",
    "\n",
    "bart_model.eval()\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"BART Generating\"):\n",
    "    pred = generate_summary(\n",
    "        model=bart_model,\n",
    "        encoder_tokenizer=bart_tokenizer,\n",
    "        decoder_tokenizer=bart_tokenizer,\n",
    "        text=row[\"dialogue\"],\n",
    "        device=device,\n",
    "        max_source_len=MAX_SOURCE_LEN,\n",
    "        max_target_len=MAX_TARGET_LEN,\n",
    "        source_prefix=\"\",\n",
    "    )\n",
    "    bart_test_predictions.append(pred)\n",
    "\n",
    "# Create results DataFrame\n",
    "bart_full_test_results = test_df.copy()\n",
    "bart_full_test_results[\"model_prediction\"] = bart_test_predictions\n",
    "\n",
    "# Save\n",
    "bart_predictions_path = BART_OUTPUT_DIR / \"test_predictions.csv\"\n",
    "bart_full_test_results.to_csv(bart_predictions_path, index=False)\n",
    "print(f\"\\nSaved {len(bart_test_predictions)} BART predictions to: {bart_predictions_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bart-verify-rouge-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify ROUGE scores match\n",
    "from src.eval.rouge_eval import compute_rouge_from_lists\n",
    "\n",
    "print(\"Verifying BART ROUGE scores on full test set predictions...\")\n",
    "\n",
    "bart_rouge_verify = compute_rouge_from_lists(\n",
    "    predictions=bart_test_predictions,\n",
    "    references=test_df[\"summary\"].tolist(),\n",
    ")\n",
    "\n",
    "print(f\"\\nBART ROUGE Scores (verification):\")\n",
    "print(f\"  ROUGE-1:   {bart_rouge_verify['rouge1']*100:.2f}\")\n",
    "print(f\"  ROUGE-2:   {bart_rouge_verify['rouge2']*100:.2f}\")\n",
    "print(f\"  ROUGE-L:   {bart_rouge_verify['rougeL']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5-section-header",
   "metadata": {},
   "source": [
    "---\n",
    "# PART B: T5\n",
    "---\n",
    "\n",
    "T5 (Text-to-Text Transfer Transformer) frames all NLP tasks as text-to-text problems.\n",
    "For summarization, we prepend `\"summarize: \"` to the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5-tokenizer-header",
   "metadata": {},
   "source": [
    "## 5B. T5 Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-tokenizer-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(T5_MODEL_NAME)\n",
    "\n",
    "print(f\"T5 Tokenizer: {t5_tokenizer.__class__.__name__}\")\n",
    "print(f\"  Vocab size: {len(t5_tokenizer):,}\")\n",
    "print(f\"  Pad token: '{t5_tokenizer.pad_token}' (id: {t5_tokenizer.pad_token_id})\")\n",
    "print(f\"  EOS token: '{t5_tokenizer.eos_token}' (id: {t5_tokenizer.eos_token_id})\")\n",
    "print(f\"  Task prefix: '{T5_PREFIX}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5-model-header",
   "metadata": {},
   "source": [
    "## 6B. Build or Load T5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-model-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "if RUN_TRAINING_T5:\n",
    "    print(\"Building fresh T5 model for training...\")\n",
    "    \n",
    "    t5_model = T5ForConditionalGeneration.from_pretrained(T5_MODEL_NAME)\n",
    "    \n",
    "    # Configure generation defaults\n",
    "    t5_model.config.max_length = MAX_TARGET_LEN\n",
    "    t5_model.config.min_length = MIN_LENGTH\n",
    "    t5_model.config.num_beams = NUM_BEAMS\n",
    "    t5_model.config.no_repeat_ngram_size = NO_REPEAT_NGRAM_SIZE\n",
    "    t5_model.config.length_penalty = LENGTH_PENALTY\n",
    "    t5_model.config.early_stopping = True\n",
    "    \n",
    "    print(f\"\\nT5 model built successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Loading T5 model from checkpoint: {T5_BEST_DIR}\")\n",
    "    t5_model = T5ForConditionalGeneration.from_pretrained(T5_BEST_DIR)\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(T5_BEST_DIR)\n",
    "    \n",
    "    # Load the prefix used during training\n",
    "    prefix_path = T5_BEST_DIR / \"source_prefix.txt\"\n",
    "    if prefix_path.exists():\n",
    "        loaded_prefix = prefix_path.read_text().strip()\n",
    "        if loaded_prefix != T5_PREFIX:\n",
    "            print(f\"  WARNING: Loaded prefix '{loaded_prefix}' differs from config '{T5_PREFIX}'\")\n",
    "            print(f\"  Using loaded prefix for consistency.\")\n",
    "            T5_PREFIX = loaded_prefix\n",
    "    \n",
    "    print(\"T5 model loaded successfully!\")\n",
    "\n",
    "# Move to device\n",
    "t5_model = t5_model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in t5_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in t5_model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5-dataset-header",
   "metadata": {},
   "source": [
    "## 7B. Prepare T5 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-dataset-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_t5(examples):\n",
    "    \"\"\"\n",
    "    Tokenize dialogues and summaries for T5.\n",
    "    T5 requires a task prefix (e.g., \"summarize: \").\n",
    "    \"\"\"\n",
    "    # Add prefix to dialogues\n",
    "    inputs = [T5_PREFIX + dialogue for dialogue in examples[\"dialogue\"]]\n",
    "    \n",
    "    # Encode dialogues with prefix\n",
    "    model_inputs = t5_tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_SOURCE_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # Encode summaries (as labels)\n",
    "    labels = t5_tokenizer(\n",
    "        text_target=examples[\"summary\"],\n",
    "        max_length=MAX_TARGET_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # Replace padding token id with -100 for loss calculation\n",
    "    labels_ids = np.array(labels[\"input_ids\"])\n",
    "    labels_ids[labels_ids == t5_tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels_ids.tolist()\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Convert DataFrames to HuggingFace Datasets\n",
    "print(\"Converting to HuggingFace Datasets...\")\n",
    "t5_train_dataset = Dataset.from_pandas(train_df[[\"dialogue\", \"summary\"]])\n",
    "t5_val_dataset = Dataset.from_pandas(val_df[[\"dialogue\", \"summary\"]])\n",
    "t5_test_dataset = Dataset.from_pandas(test_df[[\"dialogue\", \"summary\"]])\n",
    "\n",
    "# Tokenize\n",
    "print(f\"Tokenizing datasets for T5 (with prefix '{T5_PREFIX}')...\")\n",
    "t5_tokenized_train = t5_train_dataset.map(\n",
    "    preprocess_t5,\n",
    "    batched=True,\n",
    "    remove_columns=[\"dialogue\", \"summary\"],\n",
    "    desc=\"Tokenizing train\",\n",
    ")\n",
    "\n",
    "t5_tokenized_val = t5_val_dataset.map(\n",
    "    preprocess_t5,\n",
    "    batched=True,\n",
    "    remove_columns=[\"dialogue\", \"summary\"],\n",
    "    desc=\"Tokenizing validation\",\n",
    ")\n",
    "\n",
    "t5_tokenized_test = t5_test_dataset.map(\n",
    "    preprocess_t5,\n",
    "    batched=True,\n",
    "    remove_columns=[\"dialogue\", \"summary\"],\n",
    "    desc=\"Tokenizing test\",\n",
    ")\n",
    "\n",
    "print(f\"\\nT5 tokenized datasets:\")\n",
    "print(f\"  Train: {len(t5_tokenized_train):,} examples\")\n",
    "print(f\"  Validation: {len(t5_tokenized_val):,} examples\")\n",
    "print(f\"  Test: {len(t5_tokenized_test):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5-collator-header",
   "metadata": {},
   "source": [
    "## 8B. T5 Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-collator-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=t5_tokenizer,\n",
    "    model=t5_model,\n",
    "    label_pad_token_id=-100,\n",
    ")\n",
    "\n",
    "# Create metrics function with T5 tokenizer\n",
    "def t5_compute_metrics(eval_pred):\n",
    "    return compute_metrics(eval_pred, t5_tokenizer)\n",
    "\n",
    "print(\"T5 data collator configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5-training-header",
   "metadata": {},
   "source": [
    "## 9B. Train T5 with Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-trainer-setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TRAINING_T5:\n",
    "    print(\"Setting up T5 Seq2SeqTrainer...\")\n",
    "    print(f\"\\nTraining configuration:\")\n",
    "    print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"  Gradient accumulation: {GRAD_ACCUM_STEPS}\")\n",
    "    print(f\"  Effective batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS}\")\n",
    "    print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"  Warmup steps: {WARMUP_STEPS}\")\n",
    "    print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "    \n",
    "    t5_training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(T5_CHECKPOINT_DIR),\n",
    "        \n",
    "        # Training\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "        \n",
    "        # Optimization\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        \n",
    "        # Evaluation & Saving\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rougeL\",  # Use ROUGE-L, not val_loss!\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        \n",
    "        # Generation during evaluation\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=MAX_TARGET_LEN,\n",
    "        generation_num_beams=NUM_BEAMS,\n",
    "        \n",
    "        # Logging\n",
    "        logging_dir=str(T5_OUTPUT_DIR / \"logs\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        \n",
    "        # Performance\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=0,\n",
    "        \n",
    "        # Reproducibility\n",
    "        seed=SEED,\n",
    "    )\n",
    "    \n",
    "    t5_trainer = Seq2SeqTrainer(\n",
    "        model=t5_model,\n",
    "        args=t5_training_args,\n",
    "        train_dataset=t5_tokenized_train,\n",
    "        eval_dataset=t5_tokenized_val,\n",
    "        tokenizer=t5_tokenizer,\n",
    "        data_collator=t5_data_collator,\n",
    "        compute_metrics=t5_compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(\n",
    "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "            early_stopping_threshold=0.0,\n",
    "        )],\n",
    "    )\n",
    "    \n",
    "    print(\"\\nT5 Trainer configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-training-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TRAINING_T5:\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING T5 TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    t5_train_result = t5_trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"T5 TRAINING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nTraining time: {t5_train_result.metrics['train_runtime']:.1f} seconds\")\n",
    "    print(f\"Final training loss: {t5_train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5-save-header",
   "metadata": {},
   "source": [
    "## 10B. Save T5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-save-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TRAINING_T5:\n",
    "    print(f\"\\nSaving best T5 model to: {T5_BEST_DIR}\")\n",
    "    t5_trainer.save_model(str(T5_BEST_DIR))\n",
    "    t5_tokenizer.save_pretrained(T5_BEST_DIR)\n",
    "    t5_trainer.save_state()\n",
    "    \n",
    "    # CRITICAL: Save the prefix used during training\n",
    "    prefix_path = T5_BEST_DIR / \"source_prefix.txt\"\n",
    "    prefix_path.write_text(T5_PREFIX)\n",
    "    print(f\"Saved T5 source prefix: '{T5_PREFIX}'\")\n",
    "    \n",
    "    print(\"T5 model and tokenizer saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5-history-header",
   "metadata": {},
   "source": [
    "## 11B. T5 Training History & Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-history-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TRAINING_T5:\n",
    "    # Extract training history from trainer state\n",
    "    t5_log_history = t5_trainer.state.log_history\n",
    "    \n",
    "    # Separate evaluation logs\n",
    "    t5_eval_logs = [log for log in t5_log_history if \"eval_loss\" in log]\n",
    "    \n",
    "    # Create history DataFrame\n",
    "    t5_history_data = []\n",
    "    for eval_log in t5_eval_logs:\n",
    "        epoch = eval_log.get(\"epoch\", 0)\n",
    "        t5_history_data.append({\n",
    "            \"epoch\": int(epoch),\n",
    "            \"train_loss\": eval_log.get(\"train_loss\", np.nan),\n",
    "            \"val_loss\": eval_log.get(\"eval_loss\", np.nan),\n",
    "            \"rouge1\": eval_log.get(\"eval_rouge1\", np.nan),\n",
    "            \"rouge2\": eval_log.get(\"eval_rouge2\", np.nan),\n",
    "            \"rougeL\": eval_log.get(\"eval_rougeL\", np.nan),\n",
    "            \"rougeLsum\": eval_log.get(\"eval_rougeLsum\", np.nan),\n",
    "        })\n",
    "    \n",
    "    t5_history_df = pd.DataFrame(t5_history_data)\n",
    "    t5_history_df.to_csv(T5_HISTORY_PATH, index=False)\n",
    "    print(f\"T5 training history saved to: {T5_HISTORY_PATH}\")\n",
    "    \n",
    "else:\n",
    "    if T5_HISTORY_PATH.exists():\n",
    "        t5_history_df = pd.read_csv(T5_HISTORY_PATH)\n",
    "        print(f\"Loaded T5 training history from: {T5_HISTORY_PATH}\")\n",
    "    else:\n",
    "        t5_history_df = None\n",
    "        print(\"No T5 training history found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-history-display-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "if t5_history_df is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"T5 TRAINING HISTORY\")\n",
    "    print(\"=\"*60)\n",
    "    display(t5_history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-curves-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "if t5_history_df is not None and len(t5_history_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax = axes[0]\n",
    "    if \"train_loss\" in t5_history_df.columns:\n",
    "        ax.plot(t5_history_df[\"epoch\"], t5_history_df[\"train_loss\"], marker=\"o\", label=\"Train Loss\")\n",
    "    ax.plot(t5_history_df[\"epoch\"], t5_history_df[\"val_loss\"], marker=\"o\", label=\"Val Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"T5 — Loss Curves\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ROUGE curves\n",
    "    ax = axes[1]\n",
    "    ax.plot(t5_history_df[\"epoch\"], t5_history_df[\"rouge1\"], marker=\"o\", label=\"ROUGE-1\")\n",
    "    ax.plot(t5_history_df[\"epoch\"], t5_history_df[\"rouge2\"], marker=\"o\", label=\"ROUGE-2\")\n",
    "    ax.plot(t5_history_df[\"epoch\"], t5_history_df[\"rougeL\"], marker=\"o\", label=\"ROUGE-L\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"ROUGE Score\")\n",
    "    ax.set_title(\"T5 — ROUGE Scores (Validation)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_path = T5_OUTPUT_DIR / \"training_curves.png\"\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"Saved T5 training curves to: {fig_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5-val-qual-header",
   "metadata": {},
   "source": [
    "## 12B. T5 Validation Qualitative Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-val-qual-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"T5 VALIDATION SET: Qualitative Examples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "qualitative_samples(\n",
    "    df=val_df,\n",
    "    model=t5_model,\n",
    "    encoder_tokenizer=t5_tokenizer,\n",
    "    decoder_tokenizer=t5_tokenizer,\n",
    "    device=device,\n",
    "    max_source_len=MAX_SOURCE_LEN,\n",
    "    max_target_len=MAX_TARGET_LEN,\n",
    "    source_prefix=T5_PREFIX,  # T5 needs the prefix!\n",
    "    n=5,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5-test-header",
   "metadata": {},
   "source": [
    "## 13B. T5 Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-test-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"T5 TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if RUN_TRAINING_T5:\n",
    "    print(\"\\nRunning T5 evaluation on test set...\")\n",
    "    t5_test_results = t5_trainer.evaluate(eval_dataset=t5_tokenized_test)\n",
    "else:\n",
    "    # Create a trainer just for evaluation\n",
    "    print(\"\\nCreating T5 trainer for evaluation...\")\n",
    "    \n",
    "    t5_eval_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(T5_OUTPUT_DIR / \"eval_temp\"),\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=MAX_TARGET_LEN,\n",
    "        generation_num_beams=NUM_BEAMS,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "    \n",
    "    t5_eval_trainer = Seq2SeqTrainer(\n",
    "        model=t5_model,\n",
    "        args=t5_eval_args,\n",
    "        tokenizer=t5_tokenizer,\n",
    "        data_collator=t5_data_collator,\n",
    "        compute_metrics=t5_compute_metrics,\n",
    "    )\n",
    "    \n",
    "    print(\"Running T5 evaluation on test set...\")\n",
    "    t5_test_results = t5_eval_trainer.evaluate(eval_dataset=t5_tokenized_test)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"T5 TEST SET RESULTS\")\n",
    "print(\"-\"*40)\n",
    "print(f\"  Loss:      {t5_test_results['eval_loss']:.4f}\")\n",
    "print(f\"  ROUGE-1:   {t5_test_results['eval_rouge1']:.2f}\")\n",
    "print(f\"  ROUGE-2:   {t5_test_results['eval_rouge2']:.2f}\")\n",
    "print(f\"  ROUGE-L:   {t5_test_results['eval_rougeL']:.2f}\")\n",
    "print(f\"  ROUGE-Lsum:{t5_test_results['eval_rougeLsum']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-test-save-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save T5 test results\n",
    "t5_test_results_df = pd.DataFrame([{\n",
    "    \"model\": \"T5\",\n",
    "    \"test_loss\": t5_test_results[\"eval_loss\"],\n",
    "    \"rouge1\": t5_test_results[\"eval_rouge1\"],\n",
    "    \"rouge2\": t5_test_results[\"eval_rouge2\"],\n",
    "    \"rougeL\": t5_test_results[\"eval_rougeL\"],\n",
    "    \"rougeLsum\": t5_test_results[\"eval_rougeLsum\"],\n",
    "}])\n",
    "\n",
    "t5_test_results_df.to_csv(T5_TEST_RESULTS_PATH, index=False)\n",
    "print(f\"\\nT5 test results saved to: {T5_TEST_RESULTS_PATH}\")\n",
    "display(t5_test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5-test-qual-header",
   "metadata": {},
   "source": [
    "## 14B. T5 Test Set Qualitative Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-test-qual-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"T5 TEST SET: Qualitative Examples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "qualitative_samples(\n",
    "    df=test_df,\n",
    "    model=t5_model,\n",
    "    encoder_tokenizer=t5_tokenizer,\n",
    "    decoder_tokenizer=t5_tokenizer,\n",
    "    device=device,\n",
    "    max_source_len=MAX_SOURCE_LEN,\n",
    "    max_target_len=MAX_TARGET_LEN,\n",
    "    source_prefix=T5_PREFIX,\n",
    "    n=5,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5-predictions-header",
   "metadata": {},
   "source": [
    "## 15B. Generate All T5 Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-predictions-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating T5 predictions for all test examples...\")\n",
    "print(\"This may take a few minutes.\\n\")\n",
    "\n",
    "t5_test_predictions = []\n",
    "\n",
    "t5_model.eval()\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"T5 Generating\"):\n",
    "    pred = generate_summary(\n",
    "        model=t5_model,\n",
    "        encoder_tokenizer=t5_tokenizer,\n",
    "        decoder_tokenizer=t5_tokenizer,\n",
    "        text=row[\"dialogue\"],\n",
    "        device=device,\n",
    "        max_source_len=MAX_SOURCE_LEN,\n",
    "        max_target_len=MAX_TARGET_LEN,\n",
    "        source_prefix=T5_PREFIX,\n",
    "    )\n",
    "    t5_test_predictions.append(pred)\n",
    "\n",
    "# Create results DataFrame\n",
    "t5_full_test_results = test_df.copy()\n",
    "t5_full_test_results[\"model_prediction\"] = t5_test_predictions\n",
    "\n",
    "# Save\n",
    "t5_predictions_path = T5_OUTPUT_DIR / \"test_predictions.csv\"\n",
    "t5_full_test_results.to_csv(t5_predictions_path, index=False)\n",
    "print(f\"\\nSaved {len(t5_test_predictions)} T5 predictions to: {t5_predictions_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-verify-rouge-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify ROUGE scores match\n",
    "print(\"Verifying T5 ROUGE scores on full test set predictions...\")\n",
    "\n",
    "t5_rouge_verify = compute_rouge_from_lists(\n",
    "    predictions=t5_test_predictions,\n",
    "    references=test_df[\"summary\"].tolist(),\n",
    ")\n",
    "\n",
    "print(f\"\\nT5 ROUGE Scores (verification):\")\n",
    "print(f\"  ROUGE-1:   {t5_rouge_verify['rouge1']*100:.2f}\")\n",
    "print(f\"  ROUGE-2:   {t5_rouge_verify['rouge2']*100:.2f}\")\n",
    "print(f\"  ROUGE-L:   {t5_rouge_verify['rougeL']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "---\n",
    "# 16. Side-by-Side Comparison: BART vs T5\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-table-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 2 — FINAL COMPARISON: BART vs T5\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Combine test results\n",
    "comparison_df = pd.concat([bart_test_results_df, t5_test_results_df], ignore_index=True)\n",
    "comparison_df = comparison_df.sort_values(\"rougeL\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nTest Set Results (sorted by ROUGE-L):\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-chart-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = comparison_df[\"model\"].tolist()\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "r1 = comparison_df[\"rouge1\"].tolist()\n",
    "r2 = comparison_df[\"rouge2\"].tolist()\n",
    "rL = comparison_df[\"rougeL\"].tolist()\n",
    "\n",
    "bars1 = ax.bar(x - width, r1, width, label=\"ROUGE-1\", color=\"#2ecc71\")\n",
    "bars2 = ax.bar(x, r2, width, label=\"ROUGE-2\", color=\"#3498db\")\n",
    "bars3 = ax.bar(x + width, rL, width, label=\"ROUGE-L\", color=\"#9b59b6\")\n",
    "\n",
    "ax.set_xlabel(\"Model\")\n",
    "ax.set_ylabel(\"ROUGE Score\")\n",
    "ax.set_title(\"Experiment 2: BART vs T5 — Test Set ROUGE Scores\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.1f}',\n",
    "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3),\n",
    "                   textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom',\n",
    "                   fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = PROJECT_ROOT / \"experiments\" / \"exp2_comparison.png\"\n",
    "fig_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"Saved comparison chart to: {fig_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-qualitative-header",
   "metadata": {},
   "source": [
    "## Same Examples: BART vs T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-qualitative-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUALITATIVE COMPARISON: Same Test Examples\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sample 3 test examples\n",
    "sample_indices = test_df.sample(3, random_state=SEED).index.tolist()\n",
    "\n",
    "for idx in sample_indices:\n",
    "    row = test_df.loc[idx]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TEST EXAMPLE (index {idx})\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n[DIALOGUE]\\n{row['dialogue'][:400]}{'...' if len(row['dialogue']) > 400 else ''}\")\n",
    "    print(f\"\\n[HUMAN SUMMARY]\\n{row['summary']}\")\n",
    "    \n",
    "    # BART prediction\n",
    "    bart_pred = bart_full_test_results.loc[idx, \"model_prediction\"]\n",
    "    \n",
    "    # T5 prediction\n",
    "    t5_pred = t5_full_test_results.loc[idx, \"model_prediction\"]\n",
    "    \n",
    "    print(f\"\\n[BART]\\n{bart_pred}\")\n",
    "    print(f\"\\n[T5]\\n{t5_pred}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 17. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 2 — FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModels Evaluated:\")\n",
    "print(f\"  BART: {BART_MODEL_NAME}\")\n",
    "print(f\"  T5:   {T5_MODEL_NAME} (prefix: '{T5_PREFIX}')\")\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Best model selection: ROUGE-L (not val_loss)\")\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"\\n  BART:\")\n",
    "print(f\"    ROUGE-1: {bart_test_results['eval_rouge1']:.2f}\")\n",
    "print(f\"    ROUGE-2: {bart_test_results['eval_rouge2']:.2f}\")\n",
    "print(f\"    ROUGE-L: {bart_test_results['eval_rougeL']:.2f}\")\n",
    "\n",
    "print(f\"\\n  T5:\")\n",
    "print(f\"    ROUGE-1: {t5_test_results['eval_rouge1']:.2f}\")\n",
    "print(f\"    ROUGE-2: {t5_test_results['eval_rouge2']:.2f}\")\n",
    "print(f\"    ROUGE-L: {t5_test_results['eval_rougeL']:.2f}\")\n",
    "\n",
    "# Determine winner\n",
    "if bart_test_results['eval_rougeL'] > t5_test_results['eval_rougeL']:\n",
    "    winner = \"BART\"\n",
    "    margin = bart_test_results['eval_rougeL'] - t5_test_results['eval_rougeL']\n",
    "else:\n",
    "    winner = \"T5\"\n",
    "    margin = t5_test_results['eval_rougeL'] - bart_test_results['eval_rougeL']\n",
    "\n",
    "print(f\"\\n  Winner: {winner} (by {margin:.2f} ROUGE-L points)\")\n",
    "\n",
    "print(f\"\\nArtifacts:\")\n",
    "print(f\"  BART model: {BART_BEST_DIR}\")\n",
    "print(f\"  BART predictions: {bart_predictions_path}\")\n",
    "print(f\"  T5 model: {T5_BEST_DIR}\")\n",
    "print(f\"  T5 predictions: {t5_predictions_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "takeaways-header",
   "metadata": {},
   "source": [
    "## 18. Key Takeaways\n",
    "\n",
    "### Architecture\n",
    "\n",
    "**BART:**\n",
    "- Denoising autoencoder pretrained by corrupting and reconstructing text\n",
    "- Naturally suited for generation tasks like summarization\n",
    "- No task prefix required\n",
    "\n",
    "**T5:**\n",
    "- Text-to-text framework that frames all tasks uniformly\n",
    "- Requires task prefix (\"summarize: \") to specify the task\n",
    "- More flexible for multi-task learning\n",
    "\n",
    "### Performance\n",
    "\n",
    "*[Fill in after training completes]*\n",
    "\n",
    "- Which model achieved better ROUGE scores?\n",
    "- How many epochs before convergence?\n",
    "- Training time comparison?\n",
    "\n",
    "### Comparison to Experiment 1 (BERT→GPT-2)\n",
    "\n",
    "Both BART and T5 should significantly outperform the custom BERT→GPT-2 architecture because:\n",
    "1. **Pretrained cross-attention**: The encoder and decoder were trained together\n",
    "2. **Purpose-built for seq2seq**: These models were designed for generation tasks\n",
    "3. **Better initialization**: No randomly initialized layers to train from scratch\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Compare with **Experiment 3** (frontier LLMs via API) as upper bound\n",
    "- Final comparison in **notebook 05_evaluation_and_comparison.ipynb**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}